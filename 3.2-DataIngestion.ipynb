{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bb2998",
   "metadata": {},
   "source": [
    "### Data Ingestion - Documentloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5e1251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='This speech txt file I am just added for test Text Loader')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader('speech.txt')\n",
    "text_document = loader.load()\n",
    "text_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dad669e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 0, 'page_label': '1'}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\\nsystematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model+ Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier October 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 1, 'page_label': '2'}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\n Wizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n20202021 2022 2023 2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\n Bard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\n PaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\\nand open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs. While\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot. Fine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc. These abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41]. Param-\\neter e fficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions. Researchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners e ffec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 2, 'page_label': '3'}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. E fficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 3, 'page_label': '4'}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other. Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs. To speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0,x) (1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product ( ⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x,W,V,b,c) = (xW + b) ⊗σ(xV + c), (2)\\nwhere X is the input of layer and l, W,b,V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x,W,V,b,c) = max(0,xW + b)⊗,\\nGEGLU (x,W,V,b,c) = GELU (xW + b) ⊗(xV + c),\\nS wiGLU(x,W,V,b,c,β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 4, 'page_label': '5'}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can di fferenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write e fficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]: Provides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]: A deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C ++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based. Classifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can a ffect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The di fference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output. Here, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an e fficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives. For\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 5, 'page_label': '6'}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting /utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of di fferent training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with di fferent\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 6, 'page_label': '7'}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting /Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning. Generating\\nreasons is possible only by using di fferent prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency: Improves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 7, 'page_label': '8'}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu- α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nh Wk\\nh T HT\\nL (3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model. The training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the e ffect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]:ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 8, 'page_label': '9'}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF (LN2(x)) (4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with di fferences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes 1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90]. To gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7×larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3 ×GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little di fferent data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM ] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 9, 'page_label': '10'}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R,S,X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-αand extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128 ×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e 17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715,Dbase = 5.8316,a = 0.5243,b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector. MLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 10, 'page_label': '11'}, page_content='3.1.2. Coding\\nCodeGen [140]: CodeGen has a similar architecture to\\nPaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\\nbeddings. The model is trained on both natural language and\\nprogramming language data sequentially (trained on the first\\ndataset, then the second, and so on) on the following datasets\\n1) PILE, 2) BIGQUERY , and 3) BIGPYTHON. CodeGen pro-\\nposed a multi-step approach to synthesizing code. The purpose\\nis to simplify the generation of long sequences where the previ-\\nous prompt and generated code are given as input with the next\\nprompt to generate the next code sequence. CodeGen open-\\nsource a Multi-Turn Programming Benchmark (MTPB) to eval-\\nuate multi-step program synthesis.\\nCodex [141]: This LLM is trained on a subset of public Python\\nGithub repositories to generate code from docstrings. Com-\\nputer programming is an iterative process where the programs\\nare often debugged and updated before fulfilling the require-\\nments. Similarly, Codex generates 100 versions of a program\\nby repetitive sampling for a given description, which produces\\na working solution for 77.5% of the problems passing unit tests.\\nIts powerful version powers Github Copilot2.\\nAlphaCode [142]: A set of large language models, ranging\\nfrom 300M to 41B parameters, designed for competition-level\\ncode generation tasks. It uses the multi-query attention [143] to\\nreduce memory and cache costs. Since competitive program-\\nming problems highly require deep reasoning and an under-\\nstanding of complex natural language algorithms, the Alpha-\\nCode models are pre-trained on filtered GitHub code in popular\\nlanguages and then fine-tuned on a new competitive program-\\nming dataset named CodeContests. The CodeContests dataset\\nmainly contains problems, solutions, and test cases collected\\nfrom the Codeforces platform3. The pre-training employs stan-\\ndard language modeling objectives, while GOLD [144] with\\ntempering [145] serves as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\nCodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with\\nshallow encoder and deep decoder, trained in multiple stages\\ninitially unimodal data (code) and later bimodal data (text-code\\npairs). Each training stage has di fferent training objectives and\\nactivates different model blocks encoder, decoder, or both ac-\\ncording to the task. The unimodal pre-training includes span\\ndenoising and CLM objectives, whereas bimodal pre-training\\nobjectives contain contrastive learning, matching, and CLM for\\ntext-code pairs. CodeT5 + adds special tokens with the text to\\nenable task modes, for example, [ CLS ] for contrastive loss,\\n[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder\\narchitecture, employing Flash attention to scale up the context\\nlength to 8k. The StarCoder trains an encoder to filter names,\\n2https://github.com/features/copilot\\n3https://codeforces.com/\\nemails, and other personal data from the training data. Its fine-\\ntuned variant outperforms PaLM, LLaMA, and LAMDA on\\nHumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge\\nGalactica [148]: A large curated corpus of human scientific\\nknowledge with 48 million papers, textbooks, lecture notes,\\nmillions of compounds and proteins, scientific websites, en-\\ncyclopedias, and more are trained using the metaseq library3,\\nwhich is built on PyTorch and fairscale [149]. The model wraps\\nreasoning datasets with the <work >token to provide step-by-\\nstep reasoning context to the model, which has been shown to\\nimprove the performance on reasoning tasks.\\n3.1.4. Dialog\\nLaMDA [150]: A decoder-only model pre-trained on pub-\\nlic dialog data, public dialog utterances, and public web doc-\\numents, where more than 90% of the pre-training data is in\\nEnglish. LaMDA is trained with the objective of producing re-\\nsponses that exhibit high levels of quality, safety, and grounded-\\nness. To achieve this, discriminative and generative fine-tuning\\ntechniques are incorporated to enhance the model’s safety and\\nquality aspects. As a result, the LaMDA models can be utilized\\nas a general language model performing various tasks.\\n3.1.5. Finance\\nBloombergGPT [151]: A non-causal decoder model trained\\nusing both financial (“FINPILE” from the Bloomberg archive)\\nand general-purpose datasets. The model’s architecture is sim-\\nilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\\neters to different blocks of the model using the approach [113].\\nFor e ffective training, BloombergGPT packs documents to-\\ngether with < |endo f text| > to use the maximum sequence\\nlength, uses warmup batch size starting from 1024 to 2048, and\\nmanually reduces the learning rate multiple times during the\\ntraining.\\nXuan Yuan 2.0 [152]: A Chinese financial chat model with\\nBLOOM’s [13] architecture trained on a combination of general\\npurpose, financial, general purpose instructions, and financial\\ninstitutions datasets. Xuan Yuan 2.0 combined the pre-training\\nand fine-tuning stages to avoid catastrophic forgetting.\\n3.2. Fine-Tuned LLMs\\nPre-trained LLMs have excellent generalization abilities to\\nunseen tasks. However, because they are generally trained with\\nthe objective of next token prediction, LLMs have limited ca-\\npacity to follow user intent and are prone to generate unethical,\\ntoxic or inaccurate responses [20]. For their e ffective utiliza-\\ntion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\\ngenerate safe responses [20], which also results in increasing\\nzero-shot, few-shot, and cross-task generalization [97, 16, 18],\\nwith minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].\\nWe review various fine-tuned LLMs and strategies for effective\\nfine-tuning in this section.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 11, 'page_label': '12'}, page_content='Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\\nModels Findings & Insights\\nT5\\n• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\\nclassification layers\\nGPT-3\\n• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\\nlearners\\nmT5\\n• Large multi-lingual models perform equivalently to single language models on downstream tasks.\\nHowever, smaller multi-lingual models perform worse\\nPanGu-α • LLMs have good few shot capabilities\\nCPM-2\\n• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\\nble to full model fine-tuning\\n• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n• Inserting prompt tokens in-between sentences can allow the model to understand relations between\\nsentences and long sequences\\n• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\\n(aggregate information with the input text) for the model\\nERNIE 3.0\\n• A modular LLM architecture with a universal representation module and task-specific representa-\\ntion module helps in the finetuning phase\\n• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\\nan efficient way to take advantage of the powerful pre-trained model\\nJurassic-1\\n• The performance of LLM is highly related to the network size\\n• To improve runtime performance, more operations can be performed in parallel (width) rather than\\nsequential (depth)\\n• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\\ncabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\\nbenefits in few-shot learning tasks\\nHyperCLOV A • By employing prompt-based tuning, the performances of models can be improved, often surpassing\\nthose of state-of-the-art models when the backward gradients of inputs are accessible\\nYuan 1.0 • The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\\nbehavior in zero-shot and few-shot learning\\nGopher • Relative encodings enable the model to evaluate for longer sequences than training.\\nERNIE 3.0 Titan\\n• Additional self-supervised adversarial loss to distinguish between real and generated text improves\\nthe model performance as compared to ERNIE 3.0\\nGPT-NeoX-20B\\n• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\\nlayers\\n• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations\\nfrom growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot\\nTable Continued on Next Page\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 12, 'page_label': '13'}, page_content='Models Findings & Insights\\nOPT • Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n• Model is prone to generate repetitive text and stuck in a loop\\nGalactica\\n• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\\ndomain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\\nresearch on LLMs\\n• A working memory token approach can achieve strong performance over existing methods on\\nmathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\\ntasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\nGLaM\\n• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\\nin each transformer layer with a mixture-of-experts (MoE)\\n• The model trained on filtered data shows consistently better performances on both NLG and NLU\\ntasks, where the effect of filtering is more significant on the former tasks\\n• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\\nthe downstream tasks\\n• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\\nthe MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\\nmance\\nLaMDA • The model can be fine-tuned to learn to call different external information resources and tools\\nAlphaCode\\n• For higher e ffectiveness and e fficiency, a transformer model can be asymmetrically constructed\\nwith a shallower encoder and a deeper decoder\\n• To achieve better performances, it is necessary to employ strategies such as massively scaling\\nupsampling, followed by the filtering and clustering of samples into a compact set\\n• The utilization of novel sampling-e fficient transformer architectures designed to facilitate large-\\nscale sampling is crucial\\n• Simplifying problem descriptions can effectively improve the model’s performance\\nChinchilla\\n• The model size and the number of training tokens should be scaled proportionately: for each dou-\\nbling of the model size, the number of training tokens should be doubled as well\\nPaLM\\n• English-centric models produce better translations when translating to English as compared to non-\\nEnglish\\n• Generalized models can have equivalent performance for language translation to specialized small\\nmodels\\n• Larger models have a higher percentage of training data memorization\\n• Performance has not yet saturated even at 540B scale, which means larger models are likely to\\nperform better\\nAlexaTM\\n• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\\ncontext than decoder-only\\n• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\\nlearning\\n• Placing layer norm at the beginning of each transformer layer improves the training stability\\nTable Continued on Next Page\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 13, 'page_label': '14'}, page_content='Models Findings & Insights\\nU-PaLM\\n• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\\ndiversity\\nUL2 • Mode switching training enables better performance on downstream tasks\\n• CoT prompting outperforms standard prompting for UL2\\nGLM-130B\\n• Pre-training data with a small proportion of multi-task instruction data improves the overall model\\nperformance\\nCodeGen\\n• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\\neration\\nLLaMA • A constant performance improvement is observed when scaling the model\\n• Smaller models can achieve good performances with more training data and computing time\\nPanGu-Σ\\n• Sparse models provide the benefits of large models at a lower computation cost\\n• Randomly Routed Experts reduces catastrophic forgetting e ffects which in turn is essential for\\ncontinual learning\\n• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\\ncost-efficient while maintaining a performance similar to the original\\nBloombergGPT\\n• Pre-training with general-purpose and task-specific data improves task performance without hurt-\\ning other model capabilities\\nXuanYuan 2.0 • Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5+\\n• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\\nfor better performance\\nStarCoder • HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2\\n• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\\nfine-tuning\\n• Model trained on unfiltered data requires fewer samples for safety alignment\\nPaLM-2\\n• Data quality is important to train better models\\n• Model and data size should be scaled with 1:1 proportions\\n• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1\\n• Increasing batch size gradually stabilizes the training without loss spikes\\n• High-quality data at the final stages of training improves the model performance\\n• Increasing model context length windows step-wise allows it to better adapt to various sequence\\nlengths\\nNemotron-40B\\n• Model aligned iteratively on synthetic data with data generated from the previously aligned model\\nachieves competitive performance\\nDeepSeek • Batch size should increase with the increase in compute budget while decreasing the learning rate\\nDeepSeek-v2\\n• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring\\na significantly smaller KV cache, therefore achieving faster data generation\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 14, 'page_label': '15'}, page_content='Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels Findings & Insights\\nT0 • Multi-task prompting enables zero-shot generalization and outperforms baselines\\n• Even a single prompt per dataset task is enough to improve performance\\nWebGPT\\n• To aid the model in e ffectively filtering and utilizing relevant information, human labelers play a\\ncrucial role in answering questions regarding the usefulness of the retrieved documents\\n• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\\nend-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n• Generating answers with references can make labelers easily judge the factual accuracy of answers\\nTk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks\\n• More tasks improve generalization whereas only increasing task instances does not help\\n• Supervised trained models are better than generalized models\\n• Models pre-trained with instructions and examples perform well for different types of inputs\\nmT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before\\n• Multi-lingual training leads to even better zero-shot generalization for both English and non-\\nEnglish\\n• Training on machine-translated prompts improves performance for held-out tasks with non-English\\nprompts\\n• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\\nother pre-trained language tasks\\nOPT-IML\\n• Creating a batch with multiple task examples is important for better performance\\n• Only example proportional sampling is not enough, training datasets should also be proportional\\nfor better generalization/performance\\n• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\\nwhereas fully supervised tasks have no effect\\n• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\\n• Only 1% reasoning data improves the performance, adding more deteriorates performance\\n• Adding dialogue data makes the performance worse\\nSparrow\\n• Labelers’ judgment and well-defined alignment rules help the model generate better responses\\n• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\\nraters\\n• The combination of reinforcement learning (RL) with reranking yields optimal performance in\\nterms of preference win rates and resilience against adversarial probing\\nFlan\\n• Finetuning with CoT improves performance on held-out tasks\\n• Fine-tuning along with CoT data improves reasoning abilities\\n• CoT tuning improves zero-shot reasoning\\n• Performance improves with more tasks\\n• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n• Improving the model’s performance with instruction tuning is compute-efficient\\n• Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder • Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\nLLaMA-2-Chat\\n• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\\nRLHF step further improves model safety and make it less prone to jailbreak attacks\\nLIMA • Less high quality data is enough for fine-tuned model generalization\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 15, 'page_label': '16'}, page_content='Figure 10: This example illustrates the PanGu- P architecture, as depicted in\\nthe image sourced from [92].\\n3.2.1. Instruction-Tuning with Manually Created Datasets\\nNumerous hand-crafted instruction-tuning datasets with\\ndifferent design choices are proposed in the literature to\\ninstruction-tune LLMs. The performance of fine-tuned LLMs\\ndepends on multiple factors, such as dataset, instruction diver-\\nsity, prompting templates, model size, and training objectives.\\nKeeping this in view, diverse fine-tuned models have emerged\\nin the literature using manually created datasets.\\nThe models T0 [17] and mT0 (multi-lingual) [154] employ\\ntemplates to convert existing datasets into prompt datasets.\\nThey have shown improvements in generalization to zero-shot\\nand held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\\nwith in-context instructions to study generalization on unseen\\ntasks when given in-context instructions during test time. The\\nmodel outperformed Instruct-GPT, despite being smaller in\\nsize, i.e., 11B parameters as compared to 175B of GPT-3.\\nIncreasing Tasks and Prompt Setups: Zero-shot and few-shot\\nperformance improves significantly by expanding task collec-\\ntion and prompt styles. OPT-IML [97] and Flan [16] curated\\nlarger 2k and 1.8k task datasets, respectively. While increasing\\ntask size alone is not enough, OPT-IML and Flan add more\\nprompting setups in their datasets, zero-shot, few-shot, and\\nCoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\\nfurther on 1.88M CoT samples. Another method [102] uses\\nsymbolic tasks with tasks in T0, Flan, etc.\\n3.2.2. Instruction-Tuning with LLMs Generated Datasets\\nGenerating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are often\\nwritten by humans, smaller in size, and less diverse. To over-\\ncome this, self-instruct [19] proposed an approach to prompt\\navailable LLMs to generate instruction-tuning datasets. Self-\\ninstruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\\ninstruction, and 1 sample per task and iteratively generates new\\ninstructions (52k) and instances (82k input-output pairs) using\\nFigure 11: An example image shows an instance of the Flan training paradigm,\\ntaken from [16].\\nGPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data\\nof datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.\\nLLaMA Tuned: Various models in the literature instruction-\\ntune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-\\nated datasets. Among these, Alpaca [158], Vicuna [159],\\nand LLaMA-GPT-4 [160] are a few general-purpose fine-tuned\\nmodels, where Alpaca is trained on 52k samples from text-\\ndavinci-003, Vicuna on 70k samples from ShareGPT.com, and\\nLLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\\n4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million\\nsamples) by generating data from ChatGPT and outperforms\\nGPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the\\nLLaMA’s consistent tokenization of numbers. HuaTuo [162] is\\na medical knowledge model, fine-tuned with a generated QA\\ndataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs\\nto convert given instructions into a more complex set. The in-\\nstructions are iteratively evolved with re-writing instructions in\\ncomplex wording and creating new instructions. With this style\\nof automated instruction generation, WizardLM [163] (fine-\\ntuned LLaMA on 250k instructions), outperforms Vicuna and\\nAlpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.\\n3.2.3. Aligning with Human Preferences\\nIncorporating human preferences into LLMs presents a\\nsignificant advantage in mitigating undesirable behaviors and\\nensuring accurate outputs. The initial work on alignment, such\\nas InstructGPT [20] aligns GPT-3 using a 3-step approach,\\ninstruction-tuning, reward modeling, and fine-tuning with\\nreinforcement learning (RL). The supervised fine-tuned GPT-3\\non demonstrations is queried to generate responses, which\\nhuman labelers rank according to human values, and a reward\\nmodel is trained on the ranked data. Lastly, the GPT-3 is trained\\nwith proximal policy optimization (PPO) using rewards on the\\ngenerated data from the reward model. LLaMA 2-Chat [21]\\nimproves alignment by dividing reward modeling into help-\\nfulness and safety rewards and using rejection sampling in\\naddition to PPO. The initial four versions of LLaMA 2-Chat\\nare fine-tuned with rejection sampling and then with PPO on\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 16, 'page_label': '17'}, page_content='top of rejection sampling.\\nAligning with Supported Evidence: This style of alignment\\nallows the model to generate responses with proofs and facts,\\nreduces hallucination, and assists humans more e ffectively,\\nwhich increases trust in the model’s output. Similar to\\nthe RLHF training style, a reward model is trained to rank\\ngenerated responses containing web citations in answers\\nto questions, which is later used to train the model, as in\\nGopherCite [165], WebGPT [166], and Sparrow [167]. The\\nranking model in Sparrow [167] is divided into two branches,\\npreference reward and rule reward, where human annotators\\nadversarial probe the model to break a rule. These two rewards\\ntogether rank a response to train with RL.\\nAligning Directly with SFT: The PPO in the RLHF pipeline\\nis complex, memory-intensive, and unstable, requiring mul-\\ntiple models, reward, value, policy, and reference models.\\nAvoiding this sophisticated alignment pipeline is possible by\\nincorporating minimal changes in the supervised fine-tuning\\n(SFT) pipeline as in [168, 169, 170], with better or compa-\\nrable performance to PPO. Direct preference optimization\\n(DPO) [168] trains a model directly on the human-preferred\\nresponses to maximize the likelihood of preferred against\\nunpreferred responses, with per-sample importance weight.\\nReward ranked fine-tuning RAFT [169] fine-tunes the model\\non ranked responses by the reward model. Preference ranking\\noptimization (PRO) [171] and RRHF [170] penalize the model\\nto rank responses with human preferences and supervised loss.\\nOn the other hand, chain-of-hindsight (CoH) [172] provides\\nfeedback to the model in language rather than reward, to learn\\ngood versus bad responses.\\nAligning with Synthetic Feedback: Aligning LLMs with\\nhuman feedback is slow and costly. The literature suggests a\\nsemi-automated process to align LLMs by prompting LLMs to\\ngenerate helpful, honest, and ethical responses to the queries,\\nand fine-tuning using the newly created dataset. Constitutional\\nAI [173] replaces human feedback in RLHF with AI, calling\\nit RL from AI feedback (RLAIF). AlpacaFarm [174] designs\\nprompts to imitate human feedback using LLMs APIs. Oppo-\\nsite to constitutional AI, AlpacaFarm injects noise in feedback\\nto replicate human mistakes. Self-Align [98] prompts the\\nLLM with ICL examples, instructing the LLM about what the\\nresponse should contain to be considered useful and ethical.\\nThe same LLM is later fine-tuned with the new dataset.\\nAligning with Prompts: LLMs can be steered with prompts to\\ngenerate desirable responses without training [175, 176]. The\\nself-correction prompting in [176] concatenates instructions\\nand CoT with questions, guiding the model to answer its\\ninstruction following a strategy to ensure moral safety before\\nthe actual answer. This strategy is shown to reduce the harm in\\ngenerated responses significantly.\\nRed-Teaming/Jailbreaking/Adversarial Attacks: LLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-\\nformation, and other shortcomings through adversarial probing.\\nThe models are susceptible to generating harmful responses\\neven though they are aligned for safety [177, 178]. Red-\\nteaming is a common approach to address illicit outputs, where\\nthe LLMs are prompted to generate harmful outputs [178, 179].\\nThe dataset collected through red-teaming is used to fine-tune\\nmodels for safety. While red-teaming largely relies on human\\nannotators, another work [180] red-team LLMs to find prompts\\nthat lead to harmful outputs for other LLMs.\\n3.2.4. Continue Pre-Training\\nAlthough fine-tuning boosts a model’s performance, it leads\\nto catastrophic forgetting of previously learned information.\\nConcatenating fine-tuning data with a few randomly selected\\npre-training samples in every iteration avoids network forget-\\nting [181, 152]. This is also e ffective in adapting LLMs for\\ncases where fine-tuning data is small and the original capac-\\nity is to be maintained. Prompt-based continued pre-training\\n(PCP) [182] trains the model with text and instructions related\\nto tasks and then finally instruction-tunes the model for down-\\nstream tasks.\\n3.2.5. Sample E fficiency\\nWhile fine-tuning data is generally many-fold smaller than\\nthe pre-training data, it still has to be large enough for accept-\\nable performance [16, 97, 18] and requires proportional com-\\nputing resources. Studying the effects on performance with less\\ndata, existing literature [183, 184] finds that models trained\\non less data can outperform models trained with more data.\\nIn [183], 25% of the total downstream data is found enough\\nfor state-of-the-art performance. Selecting coreset-based 0.5%\\nof the total instruction-tuning data improves the model perfor-\\nmance by 2% in [184], as compared to the complete data tun-\\ning. Less is more for alignment (LIMA) [185] uses only 1000\\ncarefully created demonstrations to fine-tune the model and has\\nachieved comparable performance to GPT-4.\\n3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-\\npensive attention and high memory requirements. A model\\ntrained on limited sequence lengths fails to generalize to unseen\\nlengths at inference time [186, 49]. Alternatively, LLMs with\\nALiBi [65] positional encodings can perform zero-shot length\\nextrapolation. However, ALiBi has less expressive power [66]\\nand inferior performance on multiple benchmarks [46], and\\nmany LLMs use RoPE positional embedding that is unable to\\nperform zero-shot extrapolation. A larger context length has\\nbenefits such as a better understanding of longer documents,\\nmore samples in in-context learning, execution of bigger rea-\\nsoning processes, etc. Expanding context length during fine-\\ntuning is slow, inefficient, and computationally expensive [49].\\nTherefore, researchers employ various context window extrap-\\nolation techniques discussed below.\\nPosition Interpolation: Rather than extrapolating, [49] shows\\nthat interpolating position encodings within the pre-trained con-\\ntext window are more e ffective. The work demonstrates that\\nonly 1000 steps of fine-tuning are enough to achieve better re-\\nsults on larger windows without reducing performance com-\\npared to the original context size. Giraffe [46] uses power scal-\\ning in RoPE, and YaRN [47] proposed NTK-aware interpola-\\ntion.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 17, 'page_label': '18'}, page_content='Efficient Attention Mechanism: Dense global attention is\\none of the major constraints in training larger context win-\\ndow LLMs. Using e fficient attention variants, such as lo-\\ncal, sparse, and dilated attention, reduces the computation cost\\nsignificantly. LongT5 [48] proposes transient global atten-\\ntion (TGlobal), applying attention to local and global tokens\\n(windowed token averaging). The model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on\\n4098 sequence length, fine-tunes on larger window sizes, as\\nlarge as 16k, and improves task performance on longer inputs.\\nThis shows the extrapolation ability of TGlobal attention with\\nonly fine-tuning. COLT5 [187] uses two branches, one with\\nlightweight and the other with heavyweight attention and feed-\\nforward layers. All tokens are processed from the lightweight\\nbranch, and only important tokens are routed to the heavy-\\nweight branch. LongNet [188] replaces standard attention with\\ndilated attention, expanding sequence length to 1 billion tokens.\\nLongLoRA [189] proposes shift-short attention, used during\\nfine-tuning to reduce dense attention costs. However, the model\\nduring inference uses dense attention and achieves similar per-\\nformance as full attention fine-tuning.\\nExtrapolation without Training: LM-Infinite [186] and par-\\nallel context windows (PCW) [190] show length extrapolation\\nis possible using pre-trained LLMs. LM-Infinite suggested Λ-\\nshaped attention applied within the original context window\\nlimits. Likewise, PCW chunks larger inputs into the pre-trained\\ncontext lengths and applies the same positional encodings to\\neach chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-\\nnated with the input, known as context augmentation, in-\\ncontext learning (ICL), or few-shot prompting. They show ex-\\ncellent generalization to unseen tasks with few-shot prompt-\\ning, enabling LLMs to answer queries beyond the capacity ac-\\nquired during training [6, 55]. These emergent abilities allow\\nfor adapting the model without fine-tuning—a costly process.\\nAside from this, hallucination, producing inaccurate, unsafe,\\nor factually incorrect responses, is common for LLMs, which is\\navoided by augmenting contextual data. While the user can pro-\\nvide in-context samples in the query [54, 32], here we specifi-\\ncally refer to the methods that access external storage program-\\nmatically, calling them augmented LLMs.\\nThe literature suggests various external memory designs to aug-\\nment LLMs, long-term [191, 192, 193, 194], short-term [195],\\nsymbolic [196], and non-symbolic [197, 198]. The memory\\ncan be maintained in different formats such as documents, vec-\\ntors, or databases. A few systems maintain intermediate mem-\\nory representations to retain information across multiple iter-\\nations [194, 192], while others extract important information\\nfrom the datasets and save it in memory for recall [199]. The\\nmemory read and write operations are performed either with\\nor without LLMs cooperation [192, 200, 194, 201], acting as\\na feedback signal in [195]. We discuss di fferent types of aug-\\nmented LLMs below.\\nFigure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\\ntracts a similar context to the input and forwards it to the LLM either in simple\\nlanguage or encoded through Fusion-in-Decoder (FiD). Depending on the task,\\nretrieval and generation may repeat multiple times.\\n3.4.1. Retrieval Augmented LLMs\\nLLMs may have limited memory and outdated information,\\nleading to inaccurate responses. Retrieving relevant informa-\\ntion from external up-to-date storage enables the LLMs to\\naccurately answer with references and utilize more informa-\\ntion. With retrieval augmentation, smaller models have been\\nshown to perform at par with larger models. For instance, the\\n11B model can become competitive to 540B PaLM in [25] and\\n7.5B to 280B Gopher in [193]. Retrieval augmented language\\nmodeling (RALM) has two major components, shown in\\nFigure 12, namely: 1) retriever and 2) language model. In\\nRALM, the retriever plays a crucial role in driving LLM\\nresponse, where incorrect information can steer LLMs to false\\nbehavior. This leads to the development of various methods to\\nretrieve accurate information and fuse with the query for better\\nperformance.\\nZero-Shot Retrieval Augmentation: This kind of augmen-\\ntation keeps the original LLM architecture and weights\\nunchanged and uses BM25 [202], nearest neighbors, or frozen\\npre-trained models like Bert [7] as a retriever. The retrieved\\ninformation is provided as input to the model for response\\ngeneration, shown to improve performance over LLMs without\\nretrieval [198, 203]. In some scenarios, multiple retrieval\\niterations are required to complete the task. The output\\ngenerated in the first iteration is forwarded to the retriever\\nto fetch similar documents. Forward-looking active retrieval\\n(FLARE) [197] initially generates the response and corrects\\nthe output by retrieving relevant documents if the response\\ncontains low-confidence tokens. Similarly, RepoCoder [204]\\nfetches code snippets recursively for code completion.\\nTraining with Retrieval Augmentation: To reduce failures in\\nretrieval augmentation generation (RAG), researchers train or\\nfine-tune retrievers and LLMs with a retrieval augmentation\\npipeline. We discuss the literature below based on their focus\\non the respective training processes of the pipeline.\\nTraining LLM:Retrieval-enhanced transformer (RETRO) [193]\\nshows pre-training smaller LLMs with RAG pipeline outper-\\nforms larger LLMs, such as GPT-3 trained without RAG.\\nRETRO uses a 2-trillion token subset of MassiveText as\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 18, 'page_label': '19'}, page_content='a database. The retrieval pipeline divides the input query\\ninto subsets and retrieves relevant chunks from the database\\nfor each subset, encoded together with input intermediate\\nrepresentations for generating tokens. It uses cross-chunked\\nattention to attend to previous chunks auto-regressively. A\\nstudy on RETRO [205] shows models pre-trained without RAG\\nbut fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.\\nTraining Retriever: Quality of responses generated by LLMs\\nis highly dependent on the in-context examples. There-\\nfore, [206, 207, 208, 209] train retrievers to retrieve accurate\\nfew-shot samples while keeping the LLM frozen for gener-\\nation. Retrieved samples are ranked to build ground-truth\\ndata to train retrievers with contrastive learning in [206, 208].\\nRoBERTa is trained for downstream tasks in [207] for ICL\\nsamples retrieval. REPLUG [209] trains the retriever with\\nsupervised signals from the frozen LLM-generated outputs.\\nTraining Retriever and LLM: Further benefits are achieved by\\ntraining both the retriever and the model in [25, 210, 211]. In\\nthis case, the error propagates back to the retriever, updating\\nboth the language model and the retriever. While masked\\nlanguage modeling (MLM) is a common pre-training objec-\\ntive [25, 211], retrieval pre-trained transformer (RPT) [210]\\nused document chunk prediction as a pre-training objective for\\nlong text modeling.\\nEncoded Context Augmentation: Concatenating retrieved\\ndocuments with the query becomes infeasible as the sequence\\nlength and sample size grow. Encoding the context and fusing\\nit with the decoder (Fusion-in-Decoder) using cross-attention\\nmakes it possible to augment more samples without increasing\\ncomputation costs significantly [212, 193, 210, 25].\\nWeb Augmented: Locally stored memory, but external to\\nLLM, has limited information. However, a large amount of\\ninformation is available on the internet, which gets updated\\nregularly. Rather than storing information locally, various\\nmethods retrieve query-related context through a web search\\nand forward it to LLMs [213, 214, 166].\\n3.4.2. Tool Augmented LLMs\\nWhile RAG relies on the retriever to provide context to the\\nLLM to answer queries, tool augmented LLMs capitalize on the\\nreasoning abilities of LLMs to iteratively plan by dividing tasks\\ninto sub-tasks, selecting necessary tools, and taking actions to\\ncomplete the task [215, 216, 217, 27]. A generic pipeline of\\ntool-augmented LLMs is shown in Figure 13, where di fferent\\nmodules in Figure 13 are selected in a loop until the task com-\\npletion.\\nZero-Shot Tool Augmentation: LLMs in-context learning and\\nreasoning abilities enable them to interact with tools with-\\nout training. Automatic reasoning and tool-use (ART) [217]\\nbuilds a task library with demonstrations of reasoning steps and\\ncalling external tools. It retrieves similar task examples and\\nprovides the context to the LLM for inference. Aside from\\nthis, [218] shows tool documentation is enough to teach LLMs\\nto use tools without demonstrations. RestGPT [219] integrates\\nLLMs with RESTful APIs by decomposing tasks into planning\\nFigure 13: A basic flow diagram of tool augmented LLMs. Given an input and\\na set of available tools, the model generates a plan to complete the task. The\\ntool augmented LLMs utilize di fferent modules iteratively, such as retriever,\\ntool execution, read-write to memory, feedback, etc., depending on the task.\\nand API selection steps. The API selector understands the API\\ndocumentation to select a suitable API for the task and plan the\\nexecution. ToolkenGPT [220] uses tools as tokens by concate-\\nnating tool embeddings with other token embeddings. During\\ninference, the LLM generates the tool tokens representing the\\ntool call, stops text generation, and restarts using the tool exe-\\ncution output.\\nTraining with Tool Augmentation: LLMs are trained to inter-\\nact with diverse tools, enhancing planning abilities to overcome\\nthe limitations of zero-shot tool augmentation [221, 27, 222,\\n223]. Gorilla [221] instruction-tunes LLaMA with information\\nretrieval from API documentation. It uses the self-instruct [19]\\ndata generation pipeline with GPT-4 by providing in-context\\nexamples retrieved from API documentation. Tool augmented\\nlanguage model (TALM) [27] fine-tunes T5 [10] for tool use\\nwith a self-play approach, where it iteratively completes tool\\nmanipulation tasks and includes them back in the training set.\\nToolLLM [223] collects 16k APIs from RapidAPI. It samples\\nAPIs from the list to generate an instruction-tuning dataset us-\\ning ChatGPT in single-tool and multi-tool scenarios. For high-\\nquality datasets, ToolLLM suggested a depth-first search-based\\ndecision tree (DFSDT) method to generate ground-truths with\\ndiverse reasoning and planning.\\nMultimodal Tool Augmentation: The compositional reasoning\\ncapacity of LLMs allows them to manipulate tools in multi-\\nmodal settings [215, 216, 224]. Following the pipeline shown\\nin Figure 13, the LLM outlines a plan, generally executing in a\\nsequence: Plan →Tool selection →Execute →Inspect →\\nGenerate, to respond to the user query. Here, the database of\\ntools is rich in modalities, including text, images, etc. Many of\\nthe multimodal tool augmentation systems employ multimodal\\nLLMs [31, 225, 224, 216], while others utilize single modality\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 19, 'page_label': '20'}, page_content='LLMs and generate a plan on using di fferent modality tools to\\nsolve multimodal queries [226].\\n3.5. LLMs-Powered Agents\\nAI agents are autonomous entities, capable of planning,\\ndecision-making, and performing actions to achieve complex\\ngoals. In the early days, AI agents were rule-based, de-\\nsigned for narrow tasks, and had limited capabilities, such\\nas Clippy [227] and Deep Blue [228]. In contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it\\npossible to incorporate them in diverse applications, includ-\\ning LLMs-powered agents [224, 216], where LLMs behave\\nas the brain of agents. LLMs have been incorporated in web\\nagents [166, 167], coding agents [229], tool agents [27, 223],\\nembodied agents [26], and conversational agents [195], requir-\\ning minimal to no fine-tuning\". Below we summarize the re-\\nsearch in LLMs-based autonomous agents. For a more detailed\\ndiscussion, please refer to [230, 231].\\nLLMs Steering Autonomous Agents: LLMs are the cognitive\\ncontrollers of the autonomous agents. They generate plans, rea-\\nson about tasks, incorporate memory to complete tasks, and\\nadapt the outline depending on the feedback from the environ-\\nment. Depending on the acquired capabilities of LLMs, many\\nmethods fine-tune, propose a better prompting approach, or uti-\\nlize different modules to enhance agents’ performance. Mod-\\nules and strategies employed in autonomous agents are briefly\\ndiscussed below.\\nPlanning and Reasoning: Completing a complex task requires\\nhuman-like logical thinking, planning necessary steps, and\\nreasoning current and future directions. Prompting methods\\nlike chain-of-thoughts [103], tree-of-thoughts [105], and self-\\nconsistency [104] are central to agents, eliciting LLMs to rea-\\nson its actions and choose among di fferent paths for task com-\\npletion. When LLMs are prompted with a task description and\\na sequence of actions, they can accurately generate plan ac-\\ntions without any fine-tuning [232]. Reasoning via planning\\n(RAP) [233] incorporates a re-purposed LLM as a world model\\nto reason about future outcomes and explore alternative paths\\nfor task completion. Retroformer [234] uses a retrospective\\nLLM to improve main LLM planning and reasoning capabil-\\nities by providing helpful task cues.\\nFeedback: LLMs in open-loop systems generate plans and as-\\nsume that the agent will complete them successfully. However,\\nthe actual scenario is di fferent with failures and variable re-\\nsponses from the environment. To correctly complete tasks,\\nmany methods use LLMs in a closed-loop where the action re-\\nsponse is provided as feedback to the LLMs to re-assess and\\nupdate the plan as required [235, 236, 237, 195]. Another di-\\nrection of research exploits LLMs as reward functions to train\\nreinforcement learning (RL) policies instead of humans [238].\\nMemory: LLMs can learn from the context provided in the\\nprompt. In addition to internal memory, various systems em-\\nploy external memory to save the response history. Reflex-\\nion [195] maintains an episodic memory to use previous re-\\nsponses as feedback to improve future decision-making. Retro-\\nformer [234] improves its responses by employing short-term\\nand long-term memory, where short-term memory contains re-\\ncent responses and long-term memory keeps summarized failed\\nattempts to add in the prompt as reflection.\\nMulti-Agents Systems: LLMs can play user-defined roles and\\nbehave like a specific domain expert. In multi-agent systems,\\neach LLM is assigned a unique role, simulating human behav-\\nior and collaborating with other agents to complete a complex\\ntask [229, 239].\\nLLMs in Physical Environment: LLMs are good at\\ninstruction-following, however, utilizing them for physically\\ngrounded tasks requires adaptation, as they lack real-world\\nknowledge. This could lead to generating illogical responses\\nfor a particular physical situation [240, 26]. SayCan [240]\\nmake LLMs aware of the available low-level task operations.\\nLLM (Say) builds a high-level plan to complete the task and\\na learned affordance function (Can) explores the possibility of\\nexecuting the plan in the real world. SayCan uses RL to train\\nthe language-conditioned affordance function. PaLM-E enables\\nthe LLM to solve grounded tasks by training multi-modal LLM\\nfeeding inputs directly from the sensors.\\nManipulation: In the area of manipulation [236, 241], LLMs\\nenhance a robot’s dexterity and adaptability, excelling in tasks\\nlike object recognition, grasping, and collaboration. They ana-\\nlyze visual and spatial information to determine the most effec-\\ntive approach to interact with objects.\\nNavigation: LLMs enhance a robot’s ability to navigate com-\\nplex environments with precision and adaptability [242, 243,\\n244, 245]. They generate feasible paths and trajectories for\\nrobots, accounting for intricate environmental details [246].\\nThis ability is valuable in scenarios requiring precise and\\ndynamically adaptable navigation in environments like ware-\\nhouses, transport, healthcare facilities, and residences.\\n3.6. E fficient LLMs\\nDeploying LLMs in production is expensive. Reducing their\\nrunning costs while preserving performance is an appealing\\narea of research. This section summarizes the approaches sug-\\ngested to enhance LLMs’ efficiency.\\n3.6.1. Parameter E fficient Fine-Tuning\\nFine-tuning LLMs with tens or hundreds of billions of pa-\\nrameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\\n(540B), etc., is computationally intensive and time-consuming.\\nTo avoid complete model fine-tuning, numerous parameter-\\nefficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try\\nto achieve acceptable model fine-tuning performance at reduced\\ncosts. As compared to full fine-tuning [248], PEFT performs\\nbetter in low-resource setups, achieves comparable perfor-\\nmance on medium-resource scenarios, and performs worse than\\nfull fine-tuning under high-resource availability. An overview\\nof different PEFT approaches is shown in Figure 14.\\nAdapter Tuning: Adds a few trainable parameters within the\\ntransformer block. The adapter layer is a sequence of feature\\ndownscaling, non-linearity, and upscaling [106]. Variants of\\nadapter tuning inject adapter layers sequentially [106] and in\\nparallel [38], whereas the mixture of adapter (AdaMix) [249]\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 20, 'page_label': '21'}, page_content='Figure 14: Illustration of parameter-e fficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\\nthe adapter tuning category.\\nemploys multiple adapter modules in a single layer. AdaMix\\nroutes input instances randomly to one of the multiple down-\\nscale and upscale modules. The mixture of adapters is averaged\\nout for inference to avoid additional latency. Low-Rank Adap-\\ntation (LoRA) [250] learns low-rank decomposed matrices to\\nfreeze original weights. The learned weights are fused with the\\noriginal weights for inference, avoiding latency.\\nPrompt Tuning: Prompting is an e ffective way to adapt a\\npre-trained LLM for the downstream task. However, manual\\nprompts bring uncertainty in the model’s prediction, where a\\nchange in a single word drops the performance [247]. Prompt\\ntuning alleviates this problem by fine-tuning only 0.001%-3%\\nadditional parameters [251]. It concatenates trainable prompt\\nparameters with the model embeddings [247, 40, 251]. Task-\\nspecific fixed discrete prompts are concatenated with input em-\\nbeddings in [40]. As discrete prompts bring instability, prompts\\nare encoded through a learnable mapping in P-Tuning [247],\\nnaming continuous prompts, which are appended with the dis-\\ncrete prompts. Only the prompt encoder is trainable in the\\nmodel. In an extension of P-Tuning, continuous prompts are\\nconcatenated with each layer of the network in [251]. Progres-\\nsive prompts [252] avoid catastrophic forgetting and transfer\\npreviously learned knowledge by sequentially adding trainable\\nprompt embeddings to the previously frozen task embeddings.\\nPrefix Tuning: A set of trainable task-specific prefix vectors\\nare appended to the frozen transformer layers in prefix tun-\\ning [41]. The prefix vectors are virtual tokens attended by the\\ncontext tokens on the right. In addition, adaptive prefix tun-\\ning [253] applies a gating mechanism to control the information\\nfrom the prefix and actual tokens.\\nBias Tuning: Fine-tuning only bias terms in small to medium\\ntraining data has been found e ffective in BitFit [254]. This\\nmethod achieves full fine-tuning performance for tasks with less\\ntraining data and comparable performance with more training\\ndata.\\n3.6.2. Quantization\\nLLMs require extensive computing and memory for infer-\\nence. Deploying a 175B parameter GPT-3 model needs at\\nleast five 80GB A100 GPUs and 350GB of memory to store in\\nFP16 format [44]. Such demanding requirements for deploying\\nLLMs make it harder for smaller organizations to utilize them.\\nModel compression is an effective solution but comes at the cost\\nof degraded performance, especially at large scales greater than\\n6B. These models exhibit very large magnitude outliers that do\\nnot exist in smaller models [255], making it challenging and re-\\nquiring specialized methods for quantizing LLMs [44, 256].\\nPost-Training Quantization: Minimal or no training is re-\\nquired in this type of quantization, without significantly com-\\npromising the model performance. LLM-8-bit [255] uses full-\\nprecision matrix multiplication for weights associated with out-\\nlier features and 8-bit for remaining features. The lower pre-\\ncision multiplication outputs are converted to FP-16 and con-\\ncatenated with others. The quantized models have homogenous\\nword embeddings, which may degrade their performance. To\\nfix this, token-level knowledge distillation is employed in [45]\\nalong with independent quantization scaling factors for each\\nmodule due to varying weight distribution. Feature distribu-\\ntions are asymmetric and appear in di fferent channels; outlier\\nsuppression [257] shifts and scales per-channel activation dis-\\ntributions for effective quantization. SmoothQuant [44] quan-\\ntizes activations and weights to INT8 format by smoothing\\nactivations and migrating the quantization di fficulty toward\\nweights. It multiplies the inverse of the smoothing factor with\\nweights, which introduces a few outliers in the weights but is\\neasier to quantify than unsmoothed activations. OPTQ [256]\\nuses the optimal brain compression (OBC) [258] algorithm to\\nquantize the model layer-by-layer and update weights to com-\\npensate for quantization error. To improve speed and per-\\nformance, OPTQ updates weights in arbitrary order, employs\\nlazy updates, and uses better Cholesky kernels. Outlier-aware\\nweight quantization (OWQ) [259] uses the OPTQ algorithm for\\nquantization but assigns higher precision to vulnerable weights,\\ncausing outliers and lower precision for others.\\nQuantization-Aware Training: To compensate for perfor-\\nmance degradation, a quantized model is fine-tuned in\\nquantization-aware training (QAT) [260, 261, 262]. Al-\\npha Tuning quantizes the model using binary coding quan-\\ntization (BCQ) [263] and fine-tunes only quantization scal-\\ning factors. This approach improves performance over\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 21, 'page_label': '22'}, page_content='parameter-efficient fine-tuning of the pre-trained model. Sim-\\nilarly, parameter-e fficient and quantization-aware adaptation\\n(PEQA) [264] reduces the precision of fully-connected layers\\nand fine-tunes only quantization scaling parameters. LLM-\\nQAT [262] generates training data from the pre-trained network\\nand trains a quantized student model with knowledge distilla-\\ntion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM\\nwith LoRA [250] using a 4-bit normal float, which shows better\\nperformance over a 4-bit integer and float.\\n3.6.3. Pruning\\nPruning is an alternative approach to quantization to com-\\npress model size, thereby reducing LLMs deployment costs\\nsignificantly. Compared to task-agnostic pruning, task-specific\\npruning is easily achievable with good performance, where a\\nmodel is fine-tuned on the downstream task and pruned for\\nfaster inference. It is possible to prune LLMs for individual\\ntasks, but the cost of pruning and deploying task-specific mod-\\nels is high. To overcome this, many structured and unstructured\\npruning methods for LLMs have been proposed to maintain rea-\\nsonable performance across all tasks while shrinking the model\\nsize [265, 42, 266].\\nUnstructured Pruning: This kind of pruning removes less im-\\nportant weights without maintaining any structure. Existing\\nLLM pruning methods take advantage of the unique charac-\\nteristics of LLMs, uncommon for smaller models, where a\\nsmall subset of hidden states are activated with large magni-\\ntude [255]. Pruning by weights and activations (Wanda) [265]\\nprunes weights in every row based on importance, calculated\\nby multiplying the weights with the norm of input. The pruned\\nmodel does not require fine-tuning, thereby saving computa-\\ntional costs. Outlier weighed layerwise sparsity (OWL) [267]\\nextends Wanda with non-uniform layer pruning. It shows that\\nthe number of outliers varies for different layers; therefore, the\\nmodel should have variable pruning ratios for better perfor-\\nmance for every layer. Contrastive pruning (CAP) [43] itera-\\ntively prunes the model by training the sparse model using con-\\ntrastive loss between pre-trained, fine-tuned, and snapshots of\\nprevious sparse models to learn task-specific and task-agnostic\\nknowledge.\\nStructured Pruning: Here, the parameters are removed in\\ngroups, rows, columns, or matrices, which speeds up the\\ninference because of e ffective hardware tensor core utiliza-\\ntion [265]. LLM-Pruner [42] employs a 3-stage structured\\npruning strategy, identifying the groups of hidden states caus-\\ning each other to activate during the forward-pass, keeping im-\\nportant groups and removing less important ones, and fine-\\ntuning the pruned model with LoRA. Sparsity-induced mask\\nlearning (SIMPLE) [268] prunes the network using learnable\\nmasks. Similarly, another method prunes LLMs by learning\\nmasks and removing unimportant rank-1 components of the\\nfactorized weight matrix [266].\\n3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-\\ning applications, an increasing number of research works are\\nnow facilitating LLMs to perceive different modalities of infor-\\nmation like image [269, 270, 271], video [272, 273, 274], au-\\ndio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present\\nsubstantial benefits compared to standard LLMs that process\\nonly text. By incorporating information from various modal-\\nities, MLLMs can achieve a deeper understanding of context,\\nleading to more intelligent responses infused with a variety of\\nexpressions. Importantly, MLLMs align closely with human\\nperceptual experiences, leveraging the synergistic nature of our\\nmultisensory inputs to form a comprehensive understanding of\\nthe world [276, 26]. Coupled with a user-friendly interface,\\nMLLMs can offer intuitive, flexible, and adaptable interactions,\\nallowing users to engage with intelligent assistants through a\\nspectrum of input methods. According to the ways of construct-\\ning models, current MLLMs can be generally divided into three\\nstreams: pre-training, fine-tuning, and prompting. In this sec-\\ntion, we will discuss more details of these main streams, as well\\nas the important application of MLLMs in visual reasoning.\\nPre-training: This stream of MLLMs intends to support differ-\\nent modalities using unified end-to-end models. For instance,\\nFlamingo [269] applies gated cross-attention to fuse vision and\\nlanguage modalities, which are collected from pre-trained and\\nfrozen visual encoder and LLM, respectively. Moreover, BLIP-\\n2 [270] proposes a two-stage strategy to pre-train a Querying\\nTransformer (Q-Former) for the alignment between vision and\\nlanguage modalities: in the first stage, vision-language repre-\\nsentation learning is bootstrapped from a frozen visual encoder;\\nand in the second stage, a frozen LLM bootstraps vision-to-\\nlanguage generative learning for zero-shot image-to-text gen-\\neration. Similarly, MiniGPT-4 [277] deploys pre-trained and\\nfrozen ViT [278], Q-Former and Vicuna LLM [159], only train-\\ning the linear projection layer for vision and language modali-\\nties alignment.\\nFine-tuning: Derived from instruction tuning [16] for NLP\\ntasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\\nusing multimodal instructions. Following this method, LLMs\\ncan be easily and e ffectively extended as multimodal chat-\\nbots [277, 271, 29] and multimodal task solvers [279, 30, 280].\\nThe key issue of this stream of MLLMs is to collect multi-\\nmodal instruction-following data for fine-tuning [58]. To ad-\\ndress this issue, the solutions of benchmark adaptation [279,\\n281, 282], self-instruction [19, 31, 283], and hybrid composi-\\ntion [284, 280] are employed, respectively. To mitigate the gap\\nbetween the original language modality and additional modal-\\nities, the learnable interface is introduced to connect di ffer-\\nent modalities from frozen pre-trained models. Particularly,\\nthe learnable interface is expected to work in a parameter-\\nefficient tuning manner: e.g., LLaMA-Adapter [285] applies\\nan e fficient transformer-based adapter module for training,\\nand LaVIN [284] dynamically learns the multimodal feature\\nweights using a mixture-of-modality adapter. Di fferent from\\nthe learnable interface, the expert models can directly convert\\nmultimodalities into language: e.g., VideoChat-Text [272] in-\\ncorporates Whisper [286], a speech recognition expert model,\\nto generate the captions of given videos for the understanding\\nof following LLMs.\\nPrompting: Different from the fine-tuning technique that\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 22, 'page_label': '23'}, page_content='directly updates the model parameters given task-specific\\ndatasets, the prompting technique provides certain context, ex-\\namples, or instructions to the model, fulfilling specialized tasks\\nwithout changing the model parameters. Since prompting can\\nsignificantly reduce the need for large-scale multimodal data,\\nthis technique is widely used to construct MLLMs. Particularly,\\nto solve multimodal Chain of Thought (CoT) problems [103],\\nLLMs are prompted to generate both the reasoning process and\\nthe answer given multimodal inputs [287]. On this front, differ-\\nent learning paradigms are exploited in practice: for example,\\nMultimodal-CoT [287] involves two stages of rationale genera-\\ntion and answer inference, where the input of the second stage\\nis a combination of the original input and the output of the first\\nstage; and CoT-PT [288] applies both prompt tuning and spe-\\ncific visual bias to generate a chain of reasoning implicitly. In\\naddition to CoT problems, LLMs can also be prompted with\\nmultimodal descriptions and tools, effectively dividing complex\\ntasks into sub-tasks [289, 290].\\nVisual Reasoning Application: Recent visual reasoning sys-\\ntems [291, 292, 216, 293] tend to apply LLMs for better visual\\ninformation analysis and visual-language integration. Di ffer-\\nent from previous works [294, 295] that rely on limited VQA\\ndatasets and small-scale neural networks, current LLM-aided\\nmethods offer benefits of stronger generalization ability, emer-\\ngent ability, and interactivity [58]. To realize visual reasoning\\nwith the help of LLMs, prompting and fine-tuning techniques\\ncan also be utilized: for example, PointClip V2 [292] applies\\nLLMs to generate 3D-specific prompts, which are encoded as\\ntextual features and then combined with visual features for\\n3D recognition; and GPT4Tools [31] employs LoRA [250] to\\nfine-tune LLMs following tool-related instructions. Serving\\nas a controller [293], decision maker [296], or semantics re-\\nfiner [291, 297], LLMs significantly facilitates the progress of\\nvisual reasoning research.\\n3.8. Summary and Discussion\\n3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-\\ntecture and training strategies have a big impact on performance\\nand stability. Here, we summarize key architectural modules\\nused in various LLMs, leading to better performance, reduced\\ntraining time and memory, and better training stability.\\nLayer Normalization: The performance and training stability\\nof LLMs are affected significantly by layer normalization. Pre-\\nnorm, that is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [6, 127, 108].\\nBLOOM [13] and AlexaTM [122] utilize an additional layer\\nnormalization before embedding layer to stabilize the training\\nof large-scale models, while the model’s zero-shot generaliza-\\ntion ability can be negatively impacted [13]. However, another\\nstudy [33] finds that pre-norm degrades fine-tuned model per-\\nformance as compared to post-norm, and there are no stability\\nbenefits of pre-norm beyond the 100B scale. Therefore, GLM-\\n130B [33] used deep-norm which is a variant of post-norm for\\nbetter downstream task performance after fine-tuning.\\nPositional Encoding: Like other building blocks of the model,\\npositional encoding also a ffects the performance and training\\nstability of LLMs. BLOOM [13] finds ALiBi outperforms\\nlearned and rotary positional encodings. Contrary to this,\\nGLM-130B [33] identifies rotary positional encoding as being\\nbetter than ALiBi. So, there is no conclusion in the literature\\nabout positional encodings yet.\\nParallel Attention: In this type of attention, feed-forward and\\nattention layers are parallel to each other rather than sequen-\\ntial in a transformer block. It has been shown to reduce train-\\ning time by 15%. There is no evidence of performance drop\\ndue to this change in the literature and it is used by the models\\nPaLM [15], GPT-NeoX [118], and CodeGen [140].\\nMulti-Query Attention It has shared key and value attention\\nheads in a transformer block while query attention heads are\\nprojected as usual. This reduces memory usage and speeds up\\nsampling in autoregressive decoding. No performance degrada-\\ntion has been observed with this change and it makes the train-\\ning efficient allowing larger batch sizes. Multi-query attention\\nis used in [15, 142].\\nMixture of Experts: This type of architecture enables eas-\\nily scaling models to trillions of parameters [92, 91]. Only a\\nfew experts are activated during the computation making them\\ncompute-efficient. The performance of MoE models is better\\nthan dense models for the same amount of data and requires less\\ncomputation during fine-tuning to achieve performance similar\\nto dense models as discussed in [91]. MoE architectures are\\nless prone to catastrophic forgetting, therefore are more suited\\nfor continual learning [92]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [92].\\nSparse vs Dense Activated: GPT-3 [6] uses sparse transform-\\ners [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\\narchitectures to lower computational costs and increase the\\nmodel size and capacity. According to the literature, sparse\\nmodules do not degrade the model’s performance [67]. How-\\never, more experiments are required to verify this statement.\\n3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-\\ning costs, avoid loss divergence, and achieve better perfor-\\nmance. We summarize and discuss some of these key tricks\\nused in different LLMs.\\nMixed Precision: It is a famous method for LLMs to reduce\\nmemory usage and improve training e fficiency. In mixed pre-\\ncision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept\\nin FP32 format [120]. A drawback associated with this for-\\nmat change is training instability due to a smaller value range\\nresulting in loss spikes [33]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs precision-\\nsensitive operations like gradient accumulation and softmax in\\nFP32 [13]. BF16 has better performance and training stability\\nbut uses more memory and is supported on specific hardware,\\nfor example, A100 GPUs. Therefore, its adoption in LLMs is\\nlimited.\\nTraining Instability: Loss divergence or spiking is a common\\nissue in LLMs that occurs multiple times during training. This\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 23, 'page_label': '24'}, page_content='happens in the presence of gradient clipping [15]. To mitigate\\nthis problem, many approaches suggest restarting training from\\nan earlier checkpoint [15, 33, 91], skipping 200-500 earlier\\ndata batches at the point of divergence in [15] and re-shu ffling\\nbatches in [91]. The embedding layer gradient shrink proves to\\nfurther stabilize the training as its gradient norm is significantly\\nlarger than the other layers [33]. Another suggestion to improve\\ntraining stability for larger models is not to use biases in dense\\nand norm layers as in [15].\\nWeight Initialization: It plays a significant role in model con-\\nvergence and training stability. GPT-NeoX [118] initializes\\nfeed-forward layers before residuals with 2\\nL\\n√\\nd as in [153] and\\nother layers with the small initialization scheme [298]. This\\navoids activations growing exponentially with increasing depth.\\nMT-NLG [117] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [298]. Various models perform random weight initial-\\nization which can cause bad initialization, Galactica [148] sug-\\ngests a longer warmup to negate the effect.\\nLearning Rate: A suitable learning rate is important for sta-\\nble training. It is suggested to use a lower value [13, 15, 124]\\nwith warmup and decay (cosine or linear). Usually, the learn-\\ning rate is within the range 1 e−4 to 8e−4. Moreover, MT-NLG\\n(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\\ning learning rates based on the model size using the GPT-3 [6]\\nmodels ranging between 13B and 175B. This avoids tuning the\\nlearning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,\\npipeline, and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\\nIn addition to 3D parallelism, BLOOM [13] uses a zero op-\\ntimizer [37] to shard optimizer states. PanGu- α [108] and\\nPanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\\nlelism which additionally contains optimizer parallelism and\\nrematerialization.\\nMode Switching: It adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve downstream task performance\\nin [125, 124, 122]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation: Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-3 [6]\\nand other LLMs use in-context learning to control generated\\ntext. While in-context learning helps in controlling the gener-\\nated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\\nto rank its generated text for credibility and soft prompts such as\\ngenre, topic, keywords, sentiment, and length for better control\\non generated text.\\n3.8.3. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing di-\\nverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks by\\na large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance di fference between zero-shot and few-shot is\\nlarge for pre-trained models [6, 15], naming LLMs as meta-\\nlearners [6]. LLMs zero-shot evaluations underperform unsu-\\npervised methods in neural machine translation [6]. The liter-\\nature shows pre-training is not enough for good zero-shot per-\\nformance [15, 16]. To improve the zero-shot performance the\\nliterature suggests using instruction fine-tuning that improves\\nthe zero-shot performance significantly and outperforms base-\\nlines. Instruction fine-tuning has also been shown to improve\\nzero-shot generalization to unseen tasks. Another model, Flan-\\nPaLM [16], unlocks zero-shot reasoning with CoT training.\\n3.8.5. Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for di fferent\\ntasks, for example, encoder-only for NLU tasks, decoder-only\\nfor NLG, and encoder-decoder for sequence2sequence model-\\ning. Encoder-only models are famous for smaller models such\\nas Bert [7], RoBERTa [299], etc., whereas LLMs are either\\ndecoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\\nWhile decoder-only models are good at NLG tasks, various\\nLLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\\nLLaMA [156], are decoder-only models with significant per-\\nformance gains on both NLU and NLG tasks. In contradic-\\ntion to this, T5 [10] and UL2 [125] identify encoder-decoder\\nmodels out-performing decoder-only models. In another study,\\nPaLM [15] finds increasing the size of decoder-only models\\ncan reduce the performance gap between decoder-only and\\nencoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [125, 122] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5 + [34]\\nuses an encoder-decoder architecture with multiple training ob-\\njectives for different tasks, activating the encoder, decoder, or\\nboth according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\n4. Model Configurations\\nWe provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,\\nparallelism, etc in Table 3 and Table 4. Architecture details\\nof pre-trained LLMs are available in Table 5. Providing these\\ndetails for instruction-tuned models is unnecessary because it\\nfine-tunes pre-trained models for instruction datasets. Hence,\\narchitectural details are the same as the baselines. Moreover,\\noptimization settings for various LLMs are available in Table 6\\nand Table 7. We do not include details on precision, warmup,\\nand weight decay in Table 7. These details are not as important\\nas others to mention for instruction-tuned models, and are not\\nprovided by the papers.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 24, 'page_label': '25'}, page_content=\"Table 3: Summary of pre-trained LLMs ( >10B). Only the LLMs discussed individually in the previous sections are summarized. “Data /Tokens” is the model’s\\npre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\\n(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs /TPUs\\nhourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\\nre-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\\n(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,\\n“DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\\nModels PublicationVenue LicenseType ModelCreatorsPurposeNo. ofParamsCommercialUse StepsTrainedData/TokensDataCleaning No. ofProcessing UnitsProcessingUnit TypeTrainingTimeCalculatedTrain. CostTrainingParallelism Library\\nT5 [10] JMLR'20Apache-2.0GoogleGeneral11B ✓ 1M 1T Heur+Dedup 1024 TPU v3 - - D+M Mesh TensorFlowGPT-3 [6] NeurIPS'20 - OpenAIGeneral175B × - 300BDedup+QF - V100 - - M -mT5 [11] NAACL'21Apache-2.0GoogleGeneral13B ✓ 1M 1T - - - - - - -PanGu-α[108] arXiv'21 Apache-2.0HuaweiGeneral200B ✓ 260k1.1TBHeur+Dedup 2048 Ascend 910- - D+OP+P+O+R MindSporeCPM-2 [12] AI Open'21MIT TsinghuaGeneral198B ✓ 1M 2.6TB Dedup - - - - D+M JAXFormerCodex [141] arXiv'21 - OpenAICoding12B × - 100B Heur - - - - - -ERNIE 3.0 [110]arXiv'21 - BaiduGeneral10B × 120k∗ 375BHeur+Dedup 384 V100 - - M∗ PaddlePaddleJurassic-1 [112]White-Paper'21Apache-2.0AI21 General178B ✓ - 300B - 800 GPU - - D+M+P Megatron+DSHyperCLOV A [114]EMNLP'21 - NaverGeneral82B × - 300BClf+Dedup+PF 1024 A100 321h 1.32 Mil M MegatronYuan 1.0 [115]arXiv'21 Apache-2.0- General245B ✓ 26k∗ 180BHeur+Clf+Dedup 2128 GPU - - D+T+P -Gopher [116]arXiv'21 - GoogleGeneral280B × - 300BQF+Dedup 4096 TPU v3920h13.19 MilD+M JAX+HaikuERNIE 3.0 Titan [35]arXiv'21 - BaiduGeneral260B × - 300BHeur+Dedup - Ascend 910- - D+M+P+D* PaddlePaddleGPT-NeoX-20B [118]BigScience'22Apache-2.0EleutherAIGeneral20B ✓ 150k825GB None 96 40G A100- - M Megatron+DS+PyTorchOPT [14] arXiv'22 MIT Meta General175B ✓ 150k180B Dedup 992 80G A100- - D+T MegatronBLOOM [13]arXiv'22 RAIL-1.0BigScienceGeneral176B ✓ - 366BDedup+PR 384 80G A1002520h3.87 MilD+T+P Megatron+DSGalactica [148]arXiv'22 Apache-2.0Meta Science120B × 225k106B Dedup 128 80GB A100- - - MetaseqGLaM [91] ICML'22 - GoogleGeneral1.2T × 600k∗ 600B Clf 1024 TPU v4 - - M GSPMDLaMDA [150]arXiv'22 - GoogleDialog137B × 3M 2.81T Filtered 1024 TPU v31384h4.96 MilD+M LingvoMT-NLG [117]arXiv'22 Apache-v2.0MS.+NvidiaGeneral530B × - 270B - 4480 80G A100- - D+T+P Megatron+DSAlphaCode [142]Science'22Apache-v2.0GoogleCoding41B ✓ 205k967BHeur+Dedup - TPU v4 - - M JAX+HaikuChinchilla [96]arXiv'22 - GoogleGeneral70B × - 1.4T QF+Dedup - TPUv4 - - - JAX+HaikuPaLM [15] arXiv'22 - GoogleGeneral540B × 255k780B Heur 6144 TPU v4 - - D+M JAX+T5XAlexaTM [122]arXiv'22 Apache v2.0AmazonGeneral20B × 500k1.1T Filtered 128 A100 2880h1.47 Mil M DSU-PaLM [124]arXiv'22 - GoogleGeneral540B × 20k - - 512 TPU v4120h 0.25 Mil - -UL2 [125] ICLR'23 Apache-2.0GoogleGeneral20B ✓ 2M 1T - 512 TPU v4 - - M JAX+T5XGLM [33] ICLR'23 Apache-2.0MultipleGeneral130B × - 400B - 768 40G A1001440h3.37 Mil M -CodeGen [140]ICLR'23 Apache-2.0SalesforceCoding16B ✓ 650k577BHeur+Dedup - TPU v4 - - D+M JAXFormerLLaMA [127]arXiv'23 - Meta General65B × 350k1.4TClf+Heur+Dedup 2048 80G A100504h 4.12 MilD+M xFormersPanGuΣ[92] arXiv'23 - HuaweiGeneral1.085T × - 329B - 512 Ascend 9102400h - D+OP+P+O+R MindSporeBloombergGPT [151]arXiv23 - BloombergFinance50B × 139k569B Dedup 512 40G A1001272h1.97 Mil M PyTorchXuan Yuan 2.0 [152]arXiv23 RAIL-1.0Du XiaomanFinance176B ✓ - 366B Filtered - 80GB A100- - P DSCodeT5+[34] arXiv'23 BSD-3SalesforceCoding16B ✓ 110k51.5B Dedup 16 40G A100- - - DSStarCoder [147]arXiv'23OpenRAIL-MBigCodeCoding15.5B ✓ 250k 1T Dedup+QF+PF 512 80G A100624h 1.28 MilD+T+P Megatron-LMLLaMA-2 [21]arXiv'23 LLaMA-2.0Meta General70B ✓ 500k 2TMinimal Filtering- 80G A1001.7Mh - - -PaLM-2 [123]arXiv'23 - GoogleGeneral- × - - Ddedup+PF+QF - - - - - -LLaMA-3.1 [130]arXiv'24 LLaMA-3.0Meta General405B ✓ 1.2M15T Dedup+QF 16k 80G H10030.84Mh- D+T+P+C PyTorchMixtral 8x22B [131]web'24 Apache-2.0Mistral AIGeneral141B ✓ - - - - - - - - -Snowflake Arctic [132]web'24 Apache-2.0SnowflakeGeneral480B ✓ - 3.5T - - - - T+P DSNemotron-4 340B [137]web'24 Nvidia NvidiaGeneral340B ✓ - 9T - 6144 80G H100- - D+T+P -DeepSeek [138]arXiv'24 MIT DeepSeekGeneral67B ✓ - 2T Dedup+QF - - 300.6Kh- D+T+P DSDeepSeek-v2 [139]arXiv'24 MIT DeepSeekGeneral67B ✓ - 8.1T QF - H800172.8Kh- D+P HAI-LLM\\nTable 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number\\nof training samples.\\nModels PublicationVenue LicenseType ModelCreatorsPurposeNo. ofParamsCommercialUse Pre-trainedModelsStepsTrainedData/Tokens No. ofProcessing UnitsProcessingUnit TypeTrain.TimeCalculatedTrain. CostTrain.ParallelismLibrary\\nWebGPT [166]arXiv'21 - OpenAI General175B × GPT-3 - - - - - - - -T0 [17] ICLR'22Apache-2.0BigScienceGeneral11B ✓ T5 - 250B 512 TPU v3270h 0.48 Mil - -Tk-Instruct [18]EMNLP'22MIT AI2+ General11B ✓ T5 1000 - 256 TPU v34h 0.0036 Mil - Google T5OPT-IML [97]arXiv'22 - Meta General175B × OPT 8k 2B 128 40G A100- - D+T MegatronFlan-U-PaLM [16]ICLR'22Apache-2.0Google General540B ✓ U-PaLM30k - 512 TPU v4 - - - JAX+T5XmT0 [154] ACL'23Apache-2.0HuggingFace+ General13B ✓ mT5 - - - - - - - -Sparrow [167]arXiv'22 - Google Dialog70B × Chinchilla- - 64 TPU v3 - - M -WizardCoder [164]arXiv'23Apache-2.0HK Bapt.Coding15B × StarCoder200 S-78k - - - - - -Alpaca [158]Github'23Apache-2.0StanfordGeneral13B ✓ LLaMA3-EpochS-52k 8 80G A1003h 600 FSDP PyTorchVicuna [159]Github'23Apache-2.0LMSYS General13B ✓ LLaMA3-EpochS-125k - - - - FSDP PyTorchLIMA [185] arXiv'23 - Meta+ General65B - LLaMA15-EpochS-1000 - - - - - -Koala [300] Github'23Apache-2.0UC-BerkleyGeneral13B × LLaMA2-EpochS-472k 8 A100 6h 100 - JAX/FLAX\\n5. Datasets and Evaluation\\nGenerating training and evaluation datasets is expensive be-\\ncause of the large-scale data demand of LLMs. Hence, datasets\\nfor training and benchmarking these models are topics of key\\nimportance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\\nThe performance of LLMs largely depends on the training\\ndata’s quality, size, and diversity. Preparing training datasets\\nof high quality at a large scale is laborious. Researchers have\\nsuggested various pre-training and fine-tuning datasets to en-\\nhance LLMs capabilities. We summarize these e fforts in Ta-\\nble 8. While numerous training datasets are available in the\\nliterature, we cover the most widely used ones in our summary.\\n25\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 25, 'page_label': '26'}, page_content='Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\\nsize of hidden states.\\nModels Type TrainingObjective Attention Vocab Tokenizer Norm PE ActivationBias nL nH HS\\nT5 (11B) Enc-Dec Span CorruptionStandard 32k SentencePiecePre-RMSRelative ReLU × 24 128 1024GPT3 (175B) Causal-Dec Next TokenDense+Sparse - - Layer Learned GeLU ✓ 96 96 12288mT5 (13B) Enc-Dec Span CorruptionStandard 250k SentencePiecePre-RMSRelative ReLU - - - -PanGu-α(200B) Causal-Dec Next Token Standard 40k BPE Layer - - - 64 128 16384CPM-2 (198B) Enc-Dec Span CorruptionStandard 250k SentencePiecePre-RMSRelative ReLU - 24 64 -Codex (12B) Causal-Dec Next Token Standard - BPE+ Pre-LayerLearned GeLU - 96 96 12288ERNIE 3.0 (10B) Causal-Dec Next Token Standard - WordPiece Post-LayerRelative GeLU - 48 64 4096Jurassic-1 (178B) Causal-Dec Next Token Standard 256k SentencePiece∗ Pre-LayerLearned GeLU ✓ 76 96 13824HyperCLOV A (82B)Causal-Dec Next TokenDense+Sparse - BPE* Pre-LayerLearned GeLU - 64 80 10240Yuan 1.0 (245B) Causal-Dec Next Token Standard - - - - - - 76 - 16384Gopher (280B) Causal-Dec Next Token Standard 32k SentencePiecePre-RMSRelative GeLU ✓ 80 128 16384ERNIE 3.0 Titan (260B)Causal-Dec Next Token Standard - WordPiece Post-LayerRelative GeLU - 48 192 12288GPT-NeoX-20B Causal-Dec Next Token Parallel 50k BPE Layer Rotary GeLU ✓ 44 64 -OPT (175B) Causal-Dec Next Token Standard - BPE - - ReLU ✓ 96 96 -BLOOM (176B) Causal-Dec Next Token Standard 250k BPE Layer ALiBi GeLU ✓ 70 112 14336Galactica (120B) Causal-Dec Next Token Standard 50k BPE+custom Layer Learned GeLU × 96 80 10240GLaM (1.2T) MoE-Dec Next Token Standard 256k SentencePiece Layer Relative GeLU ✓ 64 128 32768LaMDA (137B) Causal-Dec Next Token Standard 32k BPE Layer RelativeGeGLU - 64 128 8192MT-NLG (530B) Causal-Dec Next Token Standard 50k BPE Pre-LayerLearned GeLU ✓ 105 128 20480AlphaCode (41B) Enc-Dec Next Token Multi-query 8k SentencePiece - - - - 64 128 6144Chinchilla (70B) Causal-Dec Next Token Standard 32k SentencePiece-NFKCPre-RMSRelative GeLU ✓ 80 64 8192PaLM (540B) Causal-Dec Next TokenParallel+Multi-query256k SentencePiece Layer RoPE SwiGLU × 118 48 18432AlexaTM (20B) Enc-Dec Denoising Standard 150k SentencePiecePre-LayerLearned GeLU ✓ 78 32 4096Sparrow (70B) Causal-Dec Pref.&Rule RM - 32k SentencePiece-NFKCPre-RMSRelative GeLU ✓ 16∗ 64 8192U-PaLM (540B) Non-Causal-Dec MoDParallel+Multi-query256k SentencePiece Layer RoPE SwiGLU × 118 48 18432UL2 (20B) Enc-Dec MoD Standard 32k SentencePiece - - - - 64 16 4096GLM (130B) Non-Causal-DecAR Blank InfillingStandard 130k SentencePiece Deep RoPE GeGLU ✓ 70 96 12288CodeGen (16B) Causal-Dec Next Token Parallel - BPE Layer RoPE - - 34 24 -LLaMA (65B) Causal-Dec Next Token Standard 32k BPE Pre-RMSRoPE SwiGLU - 80 64 8192PanGu-Σ(1085B) Causal-Dec Next Token Standard - BPE Fused Layer- FastGeLU - 40 40 5120BloombergGPT (50B)Causal-Dec Next Token Standard 131k Unigram Layer ALiBi GeLU ✓ 70 40 7680Xuan Yuan 2.0 (176B)Causal-Dec Next Token Self 250k BPE Layer ALiBi GeLU ✓ 70 112 14336CodeT5+(16B) Enc-Dec SC+NT+Cont.+Match Standard - Code-Specific - - - - - - -StarCoder (15.5B) Causal-Dec FIM Multi-query 49k BPE - Learned - - 40 48 6144LLaMA-2 (70B) Causal-Dec Next TokenGrouped-query32k BPE Pre-RMSRoPE SwiGLUE - - - -PaLM-2 - MoD Parallel - - - - - - - - -LLaMA-3.1 (405B)Causal-Dec Next TokenGrouped-query128k BPE Pre-RMSRoPE SwiGLU - 126 128 16384Nemotron-4 (340B)Causal-Dec Next Token Standard 256k SentencePiece - RoPE ReLU × 96 96 18432DeepSeek (67B) Causal-Dec Next TokenGrouped-query100k BBPE Pre-RMSRoPE SwiGLU - 95 64 8192DeepSeek-v2 (67B)MoE-Dec Next TokenMulti-Head Latent100k BBPE Pre-RMSRoPE SwiGLU - 60 128 5120\\n5.2. Evaluation Datasets and Tasks\\nThe evaluation of LLMs is important in gauging their profi-\\nciency and limitations. This process measures the model’s abil-\\nity to comprehend, generate, and interact with human language\\nacross a spectrum of tasks. Evaluating a language model (LM)\\nis divided into two broader categories: 1) natural language un-\\nderstanding (NLU) and 2) natural language generation (NLG).\\nIt is emphasized that tasks in NLU and NLG are softly catego-\\nrized and are often used interchangeably in the literature.\\nNatural Language Understanding: It measures the language\\nunderstanding capacity of LMs. It encompasses multiple tasks,\\nincluding sentiment analysis, text classification, natural lan-\\nguage inference (NLI), question answering (QA), common-\\nsense reasoning (CR), mathematical reasoning (MR), reading\\ncomprehension (RC), etc.\\nNatural Language Generation: It assesses the language gener-\\nation capabilities of LLMs by understanding the provided input\\ncontext. It includes tasks such as summarization, sentence com-\\npletion, machine translation (MT), dialogue generation, etc.\\nNumerous datasets are proposed for each task, evaluating\\nLLMs against different characteristics. To provide an overview\\nof evaluation datasets, we briefly discuss a few famous datasets\\nwithin each category and offer a comprehensive list of datasets\\nin Table 9. Moreover, we show a detailed overview of the train-\\ning datasets and evaluation tasks and benchmarks used by vari-\\nous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\\nble 11. We also compare the top-performing LLMs in various\\nNLP tasks in Table 12.\\n5.2.1. Multi-task\\nMMLU [307]: A benchmark that measures the knowledge\\nacquired by models during pretraining and evaluates models in\\nzero-shot and few-shot settings across 57 subjects, testing both\\nworld knowledge and problem-solving ability.\\nSuperGLUE [2]: A more challenging and diverse successor\\nto the GLUE [309] benchmark, SuperGLUE includes a variety\\nof language understanding tasks, such as question answering,\\nnatural language inference, and co-reference resolution. It is\\ndesigned to provide a rigorous test of language understanding\\nand requires significant progress in areas like sample-e fficient,\\ntransfer, multi-task, and unsupervised or self-supervised learn-\\ning.\\nBIG-bench [308]: The BIG-bench (Behavior of Intelligent\\nGenerative Models Benchmark) is a large-scale benchmark de-\\nsigned to test the abilities of LLMs across a wide range of\\ntasks, including reasoning, creativity, ethics, and understanding\\nof specific domains.\\nGLUE [309]: The General Language Understanding Evalua-\\ntion (GLUE) benchmark is a collection of resources for train-\\ning, evaluating, and analyzing natural language understanding\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 26, 'page_label': '27'}, page_content='Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\\nfor most of the LLMs.\\nSequence LR Optimizers Precision WeightGrad\\nModels Batch Size Length LR Warmup Decay AdaFactorAdamAdamWFP16BF16MixedDecay Clip Dropout\\nT5 (11B) 211 512 0.01 × inverse square root✓ - - - - - ✓\\nGPT3 (175B) 32K - 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nmT5 (13B) 1024 1024 0.01 - inverse square root✓ - - - - - ✓\\nPanGu-α(200B) - 1024 2e-5 - - - - - - ✓ - - - -\\nCPM-2 (198B) 1024 1024 0.001 - - ✓ - - - - - ✓\\nCodex (12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -\\nERNIE 3.0 (12B) 6144 512 1e-4 ✓ linear ✓ - - - ✓ - -\\nJurassic-1 (178B) 3.2M 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nHyperCLOV A (82B) 1024 - 6e-5 - cosine ✓ - - - ✓ - -\\nYuan 1.0 (245B) <10M 2048 1.6e-4 ✓ cosine decay to 10% ✓ - - - ✓ - -\\nGopher (280B) 3M 2048 4e-5 ✓ cosine decay to 10% ✓ ✓ - ✓ -\\nERNIE 3.0 Titan (260B) - 512 1e-4 ✓ linear ✓ ✓ ✓ ✓ -\\nGPT-NeoX-20B 1538 2048 0.97e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nOPT (175B) 2M 2048 1.2e-4 - linear ✓ ✓ ✓ ✓ ✓\\nBLOOM (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nGalactica (120B) 2M 2048 7e-6 ✓ linear decay to 10% ✓ - - - ✓ ✓ ✓\\nGLaM (1.2T) 1M 1024 0.01 - inverse square root✓ FP32+✓ - ✓ ×\\nLaMDA (137B) 256K - - - - - - - - - - - - -\\nMT-NLG (530B) 1920 2048 5e-5 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nAlphaCode (41B) 2048 1536+768 1e-4 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nChinchilla (70B) 1.5M 2048 1e-4 ✓ cosine decay to 10% ✓ ✓ - - -\\nPaLM (540B) 2048 2048 0.01 - inverse square root✓ - - - ✓ ✓ ×\\nAlexaTM (20B) 2M 1024 1e-4 - linear decay to 5% ✓ ✓ ✓ - ✓\\nU-PaLM (540B) 32 2048 1e-4 - cosine ✓ - - - - - -\\nUL2 (20B) 1024 1024 - - inverse square root- - - - - - × - -\\nGLM (130B) 4224 2048 8e-5 ✓ cosine ✓ ✓ ✓ ✓ ✓\\nCodeGen (16B) 2M 2048 5e-5 ✓ cosine ✓ - - - ✓ ✓ -\\nLLaMA (65B) 4M Tokens 2048 1.5e-4 ✓ cosine decay to 10% ✓ - - - ✓ ✓ -\\nPanGu-Σ(1.085T) 512 1024 2e-5 ✓ - ✓ ✓ - - -\\nBloombergGPT (50B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ ×\\nXuan Yuan 2.0 (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nCodeT5+(16B) 2048 1024 2e-4 - linear ✓ ✓ ✓ - -\\nStarCoder (15.5B) 512 8k 3e-4 ✓ cosine ✓ ✓ ✓ - -\\nLLaMA-2 (70B) 4M Tokens 4k 1.5e-4 ✓ cosine ✓ ✓ ✓ ✓ -\\nLLaMA-3.1 (405B) 16M 8192 8e-5 ✓ linear+cosine ✓ ✓ - - -\\nNemotron-4 (340B) 2304 4096 - - linear - - - ✓ - - ×\\nDeepSeek (67B) 4608 4096 3.2e-4 ✓ cosine ✓ ✓ ✓ ✓ -\\nDeepSeek-v2 (67B) 9216 4k 2.4e-4 ✓ step-decay ✓ - - - ✓ ✓ -\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\\nno model uses weight decay for instruction tuning.\\nSequence Optimizers Grad\\nModels Batch Size Length LR Warmup LR_Decay AdaFactorAdam AdamW Clip Dropout\\nWebGPT (175B) BC:512, RM:32 - 6e-5 - - ✓ - -\\nT0 (11B) 1024 1280 1e-3 - - ✓ - ✓\\nTk-Instruct (11B) 1024 - 1e-5 - constant - - - - -\\nOPT-IML (175B) 128 2048 5e-5 × linear ✓ ✓ ✓\\nFlan-U-PaLM (540B) 32 - 1e-3 - constant ✓ - ✓\\nSparrow (70B) RM: 8+16, RL:16 - 2e-6 ✓ cosine decay to 10% ✓ ✓ ×\\nWizardCoder (15B) 512 2048 2e-5 ✓ cosine - - - - -\\nAlpaca (13B) 128 512 1e-5 ✓ cosine - - ✓ ✓ ×\\nVicuna (13B) 128 -2048 2e-5 ✓ cosine ✓ - ×\\nLIMA (65B) 32 2048 1e-5 × linear ✓ - ✓\\nsystems. It includes a variety of tasks that test a wide range of\\nlinguistic phenomena, making it a comprehensive tool for eval-\\nuating language understanding in AI.\\n5.2.2. Language Understanding\\nWinoGrande [354]: A large-scale dataset inspired by the orig-\\ninal Winograd [357] Schema Challenge tests models on their\\nability to resolve pronoun ambiguity and encourages the devel-\\nopment of models that understand the broad context in natural\\nlanguage text.\\nCoQA [316]: A conversational question-answering dataset,\\nCoQA challenges models with questions that rely on conver-\\nsation history and require free-form text answers. Its diverse\\ncontent from seven domains makes it a rigorous test for mod-\\nels’ ability to handle a wide range of topics and conversational\\ncontexts.\\nWiC [317]: This dataset assesses a model’s ability to dis-\\ncern word meanings based on context, aiding in tasks related\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 27, 'page_label': '28'}, page_content='Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\nDataset Type Size/Samples Tasks Source Creation Comments\\nC4 [10] Pretrain 806GB - Common Crawl AutomatedA clean, multilingual dataset with billions\\nof tokens\\nmC4 [11] Pretrain 38.49TB - Common Crawl AutomatedA multilingual extension of the C4\\ndataset, mC4 identifies over 100 lan-\\nguages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301] Pretrain 825GB -\\nCommon Crawl, PubMed Central,\\nOpenWebText2, ArXiv, GitHub,\\nBooks3, and others\\nAutomatedA massive dataset comprised of 22 con-\\nstituent sub-datasets\\nROOTs [302] Pretrain 1.61TB - 498 Hugging Face datasetsAutomated46 natural and 13 programming lan-\\nguages\\nMassiveText [116] Pretrain 10.5TB - MassiveWeb, Books, News,\\nWikipedia, Github, C4 Automated99% of the data is in English\\nWikipedia [303] Pretrain - - Wikipedia AutomatedDump of wikipedia\\nRedPajama [304] Pretrain 5TB - CommonCrawl, C4, Wikipedia,\\nGithub, Books, StackExchangeAutomatedOpen-source replica of LLaMA dataset\\nPushShift.io Reddit Pretrain 21.1GB - Reddit AutomatedSubmissions and comments on Reddit\\nfrom 2005 to 2019\\nBigPython [140] Pretrain 5.5TB Coding GitHub Automated-\\nPool of Prompt (P3) [17] Instructions 12M 62 PromptSource Manual A Subset of PromptSource, created from\\n177 datasets including summarization,\\nQA, classification, etc.\\nxP3 [154] Instructions 81M 71 P3+Multilingual datasets Manual Extending P3 to total 46 languages\\nSuper-NaturalInstructions (SNI) [18]Instructions 12.4M 1616 Multiple datasets Manual Extending P3 with additional multi-\\nlingual datasets, total 46 languages\\nFlan [16] Instructions 15M 1836 Muffin+T0-SF+NIV2 Manual Total 60 languages\\nOPT-IML [97] Instructions 18.1M 1667 - Manual -\\nSelf-Instruct [19] Instructions 82k 175 - AutomatedGenerated 52k instructions with 82k sam-\\nples from 175 seed tasks using GPT-3\\nAlpaca [158] Instructions 52k - - AutomatedEmployed self-instruct method to gener-\\nate data from text-davinci-003\\nVicuna [159] Instructions 125k - ShareGPT AutomatedConversations shared by users on\\nShareGPT using public APIs\\nLLaMA-GPT-4 [160] Instructions 52k - Alpaca AutomatedRecreated Alpaca dataset with GPT-4 in\\nEnglish and Chinese\\nUnnatural Instructions [305] Instructions 68k - 15-Seeds (SNI) Automated-\\nLIMA [185] Instructions 1k - Multiple datasets Manual Carefully created samples to test perfor-\\nmance with fine-tuning on less data\\nAnthropic-HH-RLHF [306] Alignment 142k - - Manual\\nAnthropic-HH-RLHF-2 [178]Alignment 39k - - Manual\\nto Word Sense Disambiguation.\\nWikitext103 [318]: With over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as lan-\\nguage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from\\nProject Gutenberg. It is specifically designed to facilitate re-\\nsearch in unsupervised learning and language modeling, with a\\nspecial focus on long-form content.\\nC4 [10]: A clean, multilingual dataset, C4 offers billions of to-\\nkens from web-crawled data. It is a comprehensive resource for\\ntraining advanced Transformer models on various languages.\\nLCQMC [320]: The Large-scale Chinese Question Matching\\nCorpus (LCQMC) is a dataset for evaluating the performance\\nof models in semantic matching tasks. It contains pairs of ques-\\ntions in Chinese and their matching status, making it a valuable\\nresource for research in Chinese language understanding.\\n5.2.3. Story Cloze and Sentence Completion\\nStoryCloze [334]: It introduces a new “StoryCloze Test”, a\\ncommonsense reasoning framework for evaluating story under-\\nstanding, generation, and script learning. It considers a model’s\\nability to understand and generate coherent and sensible stories.\\nLAMBADA [335]: This dataset evaluates contextual text un-\\nderstanding through a word prediction task. Models must pre-\\ndict the last word of a passage, which is easy for humans when\\ngiven the whole passage, but not when given only the last sen-\\ntence.\\n5.2.4. Physical Knowledge and World Understanding\\nPIQA [340]: A dataset that probes the physical knowledge of\\nmodels, aiming to understand how well they are learning about\\nthe real world.\\nTriviaQA [341]: A dataset that tests models on reading com-\\nprehension and open domain question answering (QA) tasks,\\nwith a focus on Information Retrieval (IR)-style QA.\\nARC [342]: A larger version of the ARC-Challenge, this\\ndataset contains both easy and challenging grade-school level,\\nmultiple-choice science questions. It is a comprehensive test of\\na model’s ability to understand and answer complex questions.\\nARC-Easy [342]: A subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 28, 'page_label': '29'}, page_content='Table 9: Categorized evaluation datasets used in evaluating LLMs.\\nType Datasets/Benchmarks\\nMulti-Task MMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-\\nCLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]\\nLanguage Understanding CoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],\\nCB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\\nCLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]\\nStory Cloze and\\nSentence Completion StoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-\\nFC [312]\\nPhysical Knowledge and\\nWorld Understanding PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\\nBookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]\\nContextual Language\\nUnderstanding\\nRACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],\\ncMedQA [351],cMedQA2 [352], MATINF-QA [353]\\nCommonsense Reasoning WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\\nCLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]\\nReading Comprehension SQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],\\nCMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-\\ntiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\\nDuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD\\n1.0 [380], CAIL2018-Task1 & Task2 [381]\\nMathematical Reasoning MATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-\\nDiv [388], MAWPS [389], SV AMP [390]\\nProblem Solving HumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]\\nNatural Language Inference\\n& Logical Reasoning ANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],\\nANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-\\ngyQA [349]\\nCross-Lingual Understanding MLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-\\nGoldP [403], MLSum [404]\\nTruthfulness and Fact CheckingTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\\nBiases and Ethics in AI ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]\\nToxicity RealToxicityPrompts [413], CivilComments toxicity classification [414]\\nLanguage Translation WMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]\\nScientific Knowledge AminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral\\nGroups [148]\\nDialogue Wizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],\\nKdConv [421]\\nTopic Classification TNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]\\nIt is a great starting point for models beginning to explore ad-\\nvanced question-answering.\\nARC-Challenge [342]: A rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval, test-\\ning the true comprehension capabilities of models.\\n5.2.5. Contextual Language Understanding\\nRACE [347]: The RACE dataset is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge\\nof a real-world examination.\\nRACE-Middle [347]: Another subset of the RACE [347]\\ndataset, RACE-Middle, contains middle school-level English\\nexam questions. It offers a slightly less challenging but academ-\\nically oriented evaluation of a model’s comprehension skills.\\nRACE-High [347]: A subset of the RACE [347] dataset,\\nRACE-High consists of high school-level English exam ques-\\ntions. It is designed to evaluate the comprehension ability of\\nmodels in a more academic and challenging context.\\nQuAC [348]: This dataset simulates an information-seeking\\ndialog between students and teachers using hidden Wikipedia\\ntext. It introduces unique challenges not found in machine com-\\nprehension datasets, making it a valuable resource for advanc-\\ning dialog systems.\\n5.2.6. Commonsense Reasoning\\nHellaSwag [355]: A dataset that challenges models to pick the\\nbest ending to a context uses Adversarial Filtering to create a\\n‘Goldilocks’ zone of complexity, where generated text is absurd\\nto humans but often misclassified by models.\\nCOPA [401]: This dataset evaluates a model’s progress in\\nopen-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability to\\nunderstand and reason about cause and effect.\\nWSC [357]: The Winograd Schema Challenge (WSC) is a\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 29, 'page_label': '30'}, page_content='Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\\nis natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\\n“Mem.” is memorization.\\nBenchmark\\nModels Training Dataset BIG-\\nbenchMMLUSuper\\nGLUEQA Clf NLI MT Cloze/\\nCompletionRC CR MR Coding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5 C4 [10] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nGPT-3 Common Crawl, WebText, Books Cor-\\npora, Wikipedia\\n✓ ✓ ✓ ✓ ✓ ✓\\nmT5 mC4 [11] ✓ ✓ ✓\\nPanGu-α 1.1TB Chinese Text Corpus ✓ ✓ ✓ ✓ ✓\\nCPM-2 WuDaoCorpus [109] ✓ ✓\\nCodex 54 million public repositories from Github ✓\\nERNIE-3.0 Chinese text corpora, Baidu Search, Web\\ntext, QA-long, QA-short, Poetry and Cou-\\nplet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge\\ngraph with more than 50 million facts\\n✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nJurassic-1 Wikipedia, OWT, Books, C4, Pile [301],\\narXiv, GitHub\\n✓ ✓ ✓ ✓\\nHyperCLOV AKorean blogs, Community sites, News,\\nKiN Korean Wikipedia, Wikipedia (En-\\nglish and Japanese), Modu-Corpus: Mes-\\nsenger, News, Spoken and written lan-\\nguage corpus, Web corpus\\n✓\\nYuan 1.0 Common Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓ ✓ ✓ ✓\\nGopher subsets of MassiveWeb Books, C4, News,\\nGitHub and Wikipedia samples from Mas-\\nsiveText\\n✓ ✓ ✓ ✓ ✓ ✓ ✓\\nERNIE-3.0 TITANSame as ERNIE 3.0 and ERNIE 3.0 ad-\\nversarial dataset, ERNIE 3.0 controllable\\ndataset\\n✓ ✓ ✓ ✓ ✓\\nGPT-NeoX-20BPile [301] ✓ ✓ ✓ ✓ ✓ ✓\\nOPT RoBERTa [299], Pile [301], PushShift.io\\nReddit [423]\\n✓ ✓ ✓ ✓\\nBLOOM ROOTs [13] ✓ ✓ ✓ ✓ ✓ ✓\\nGalactica arXiv, PMC, Semantic Scholar, Wikipedia,\\nStackExchange, LibreText, Open Text-\\nbooks, RefSeq Genome, OEIS, LIPID\\nMAPS, NASAExoplanet, Common Crawl,\\nScientificCC, AcademicCC, GitHub repos-\\nitories Khan Problems, GSM8K, OneS-\\nmallStep\\n✓ ✓ ✓ ✓ ✓\\nGLaM Filtered Webpages, Social media conversa-\\ntions Wikipedia, Forums, Books, News\\n✓ ✓ ✓ ✓ ✓\\nLaMDA Infiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG Two snapshots of Common Crawl and\\nBooks3, OpenWebText2, Stack Exchange,\\nPubMed Abstracts, Wikipedia, PG-19\\n[242], BookCorpus2, NIH ExPorter, Pile,\\nCC-Stories, RealNews\\n✓ ✓ ✓ ✓ ✓\\nAlphaCode Selected GitHub repositories, CodeCon-\\ntests: Codeforces, Description2Code, Co-\\ndeNet\\n✓\\nChinchilla MassiveWeb, MassiveText Books, C4,\\nNews, GitHub, Wikipedia\\n✓ ✓ ✓ ✓ ✓ ✓\\nPaLM webpages, books, Wikipedia, news, arti-\\ncles, source code, social media conversa-\\ntions\\n✓ ✓ ✓ ✓ ✓ ✓\\nAlexaTM Wikipedia, mC4 ✓ ✓ ✓ ✓ ✓\\nU-PaLM Same as PaLM ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nUL2 - ✓ ✓ ✓ ✓ ✓ ✓\\nGLM-130B - ✓ ✓ ✓\\nCodeGen Pile, BigQuery, BigPython ✓\\nLLaMA CommonCrawl, C4, Github, Wikipedia,\\nBooks, arXiv, StackExchange\\n✓ ✓ ✓ ✓ ✓ ✓ ✓\\nPanGu-Σ WuDaoCorpora, CLUE, Pile, C4, Python\\ncode\\n✓ ✓ ✓ ✓ ✓ ✓\\nBloombergGPTinPile, Pile, C4, Wikipedia ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nCodeT5+ CodeSearchNet, Github Code ✓ ✓\\nStarCoder The Stack v1.2 ✓ ✓ ✓ ✓\\nLLaMA-2 ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nPaLM-2 Web documents, Code, Books, Maths,\\nConversation\\n✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 30, 'page_label': '31'}, page_content='Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\nModels Training Dataset BIG-\\nbench MMLUBBH RAFTFLANSNI PromptSourceTyDiQAHumanEvalMBPP\\nTruthful/\\nBias/\\nToxicity\\nT0 Pool of Prompts ✓\\nWebGPT ELI5 [424], ELI5 fact-\\ncheck [166], TriviaQA [341],\\nARC-Challenge [342], ARC-\\nEasy [342], Hand-written data,\\nDemonstrations of humans, Com-\\nparisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCTSNI [18] ✓\\nmT0 xP3 [154]\\nOPT-IML PromptSource [17], FLAN [16],\\nSNI [425], UnifiedSKG [426],\\nCrossFit [427], ExMix [428],\\nT5 [10], Reasoning\\n✓ ✓ ✓ ✓ ✓ ✓\\nFlan Muffin, T0-SF, NIv2, CoT ✓ ✓ ✓\\nWizardCoderCode Alpaca ✓ ✓\\nreading comprehension task in which a system must resolve\\nreferences in a text, often requiring world knowledge and rea-\\nsoning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering\\ndataset that requires commonsense knowledge to evaluate the\\nability of AI models to understand and answer questions.\\n5.2.7. Reading Comprehension\\nBoolQ [363]: A dataset derived from Google search queries,\\nBoolQ challenges models to answer binary (yes /no) questions.\\nThe questions are naturally occurring and are paired with a\\nparagraph from a Wikipedia article containing the answer. It\\nis a test of reading comprehension and reasoning.\\nSQUADv2 [364]: The Stanford Question Answering Dataset\\n(SQuAD) [362] is a collection of questions posed by crowd\\nworkers on a set of Wikipedia articles, where the answer to ev-\\nery question is a segment of text from the corresponding reading\\npassage. SQuADv2 combines the original SQuAD1.1 dataset\\nwith over 50,000 unanswerable questions. The aim is to evalu-\\nate a model’s ability to understand and answer questions based\\non a given context and to determine when a question is unan-\\nswerable.\\nDROP [365]: DROP, or Discrete Reasoning Over the con-\\ntent of Paragraphs, is designed to test a model’s ability to un-\\nderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]: The Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textual\\nentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\nWebQA [367]: A dataset for open-domain question answering,\\nWebQA offers a large collection of web-based question-answer\\npairs. It is designed to assess the ability of AI models to under-\\nstand and answer questions based on web content.\\nCMRC2018 [369]: This dataset is a test of Chinese language\\nmodels’ ability to reason comprehensively and is designed with\\na challenging span-extraction format that pushes the boundaries\\nof machine performance.\\n5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the\\nmathematical problem-solving abilities of AI models. It con-\\ntains a diverse set of math problems, ranging from arithmetic\\nto calculus, and is designed to test the model’s ability to under-\\nstand and solve complex mathematical problems.\\nMath23k [383]: This one challenges a model’s ability to un-\\nderstand and solve mathematical word problems. It contains\\n23,000 Chinese arithmetic word problems that require models\\nto perform reasoning and computation based on the problem\\ndescription.\\nGSM8K [384]: A dataset of diverse grade school math word\\nproblems, testing a model’s ability to perform multi-step math-\\nematical reasoning.\\n5.2.9. Problem Solving and Logical Reasoning\\nANLI [393]: A large-scale dataset designed to test the robust-\\nness of machine learning models in Natural Language Inference\\n(NLI) is created through an iterative, adversarial process where\\nhumans try to generate examples that models cannot correctly\\nclassify.\\nHumanEval [141]: A dataset for evaluating the problem-\\nsolving ability of AI models, which includes a diverse set of\\ntasks that require various cognitive abilities, making it a com-\\nprehensive tool for assessing general intelligence in AI.\\nStrategyQA [349]: A question-answering dataset that re-\\nquires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the bound-\\naries of what machines can understand and answer.\\n5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the\\nMultiNLI [429] corpus to 15 languages, including low-resource\\nones like Urdu. It tests models on cross-lingual sentence under-\\nstanding, with 112,500 annotated pairs across three categories:\\nentailment, contradiction, and neutral.\\nPAWS-X [399]:PAWS-X, or Cross-lingual Paraphrase Adver-\\nsaries from Word Scrambling, is a multilingual version of the\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 31, 'page_label': '32'}, page_content='PAWS [430] dataset for paraphrase identification. It includes\\nexamples in seven languages and is designed to evaluate the\\nperformance of cross-lingual paraphrase identification models.\\n5.2.11. Truthfulness\\nTruthful-QA [405]: A unique benchmark that measures a\\nlanguage model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some designed to test the model against com-\\nmon human misconceptions.\\n5.2.12. Biases and Ethics in AI\\nETHOS [408]: ETHOS is a hate speech detection dataset\\nbuilt from YouTube and Reddit comments. It is a tool in the\\nfight against online hate speech, offering binary and multi-label\\nvariants for robust content moderation.\\nStereoSet [409]: StereoSet is a comprehensive dataset de-\\nsigned to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. Contrasting stereotypi-\\ncal bias against language modeling ability provides a valuable\\ntool for understanding and mitigating biases in large language\\nmodels.\\n6. Applications\\nApplying Large Language Models (LLMs) to a variety of\\ndownstream tasks has become a popular trend in both AI-\\nrelated research communities and industries, with many emerg-\\ning uses being discovered and explored daily. LLMs, which are\\ncapable of understanding and generating human-like text, have\\nfound meaningful applications across a variety of fields. This\\nsection provides an overview of LLM applications in medicine,\\neducation, science, mathematics, law, finance, robotics, and\\ncoding. While each of these domains pose different challenges,\\nLLMs open up opportunities to make significant contributions\\nto these domains through their generalizability.\\nGeneral Purpose: LLMs are being widely considered as\\ngeneral-purpose tools for a wide variety of tasks [431]. This\\nis due to their inherent ability to understand, generate, and\\nmanipulate human-like text in a contextually relevant man-\\nner. This allows them to perform tasks ranging from simple\\nlanguage translation and question-answering to more complex\\ntasks like summarization, text generation, and even program-\\nming help [432]. The utility of LLMs is further enhanced by\\ntheir ability to adapt to the specific style and tone of the text\\nthey are processing, making the outputs more user-friendly and\\ncontext-aware. In everyday applications, LLMs can be used\\nas personal assistants, helping users draft emails or schedule\\nappointments [433]; they can also be deployed in customer ser-\\nvice to handle common questions or applied to generate content\\nfor digital platforms like websites by creating human-like text\\nbased on given prompts [434]. Moreover, LLMs play a cru-\\ncial role in data analysis, where they can filter large volumes of\\ntext data, summarize key points, and find patterns that would\\ntake humans much longer to identify [435]. Despite their wide-\\nranging applications, it is essential to remember that LLMs,\\nsimilar to any AI system, are only as good as the data they have\\nbeen trained on.\\nMedicine: The application of LLMs in the field of medicine is\\nreshaping healthcare delivery and research. For example, LLMs\\nare increasingly used in clinical decision support systems to\\nprovide physicians with evidence-based treatment recommen-\\ndations [436, 437, 438]. By analyzing patient data and medical\\nliterature, they can help identify potential diagnoses, suggest\\nappropriate tests, and recommend optimal treatment strategies.\\nMoreover, LLMs can also enhance patient interactions with\\nhealthcare systems; e.g., they can be used in chatbot applica-\\ntions [439, 440, 441] to answer patient queries about symptoms\\nor medications, schedule appointments, and even provide es-\\nsential health advice. For medical research, LLMs are used to\\nextract and filter information from a considerable amount of\\nmedical literature, identify relevant studies, summarize find-\\nings, and even predict future research trends [442, 443, 444].\\nFor medical education, LLMs can help create training mate-\\nrials, generate exam questions, provide detailed explanations\\nof complex medical topics, and o ffer personalized feedback to\\nstudents [445, 446, 447, 448]. They can also simulate patient\\ninteractions, enabling students to practice and improve their\\nclinical skills. At a broader level, LLMs can assist in public\\nhealth initiatives by analyzing media data to detect disease out-\\nbreaks, monitor public sentiment towards health policies, and\\ndisseminate health information in a clear and understandable\\nmanner [449]. LLMs can be employed to support public health\\ninitiatives, addressing related issues such as data privacy, the\\nnecessity for explainability, and the potential risk of propagat-\\ning biases [450, 451].\\nEducation: The integration of LLMs into the educational sec-\\ntor offers opportunities to enhance learning experiences, teacher\\nsupport, and educational content development. For students, by\\nanalyzing their learning styles, performance, and preferences,\\nLLMs can provide customized study materials and practice\\nquestions to develop personalized learning experiences [452].\\nFor teachers, LLMs can help to create lesson plans and grade\\nassignments and generate diverse and inclusive educational\\ncontent, significantly saving more time for teaching and student\\ninteraction [453, 454]. In language learning, LLMs serve as\\nadvanced conversational partners capable of simulating conver-\\nsations in multiple languages, correcting grammar, enhancing\\nvocabulary, and aiding pronunciation for the needs of fluency\\nin practice [455]. Furthermore, LLMs improve accessibility\\nin education by providing support for students with disabili-\\nties. They can generate real-time transcriptions for the hear-\\ning impaired, offer reading assistance for the visually impaired,\\nand simplify complex texts for those with learning disabili-\\nties [451]. As LLMs continue to evolve, their applications in\\neducation can benefit more students and teachers from different\\nperspectives in practice.\\nScience: Similar to medical applications, LLMs can expedite\\nthe research process by quickly analyzing and summarizing sci-\\nentific literature. By briefing comprehensible and accessible re-\\nsearch summaries, LLMs can assist researchers in staying up-\\nto-date with the latest findings, even in fields outside their area\\nof expertise [456, 457]. In addition, LLMs can aid scientists\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 32, 'page_label': '33'}, page_content='Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\\nto the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\\nbenchmark.\\nTask Dataset/Benchmark Top-1 Top-2 Top-3\\nModel (Size)Score (N-shots)Model (Size)Score (N-shots) Model (Size) Score (N-shots)\\nMulti-Task BIG-bench (B) Chinchilla (70B)65.1 (5-shot) Gopher (280B)53.97 (5-shot) PaLM (540B) 53.7 (5-shot)\\nMMLU (B) GPT-4 (-) 86.4 (5-shot) Gemini (Ultra)83.7 (5-shot)Flan-PaLM-2(f) (Large) 81.2 (5-shot)\\nLanguage UnderstandingSuperGLUE (B) ERNIE 3.0 (12B) 90.6 (-) PaLM(f) (540B) 90.4 (-) T5 (11B) 88.9 (-)\\nStory Comprehension and\\nGeneration\\nHellaSwag GPT-4 (-) 95.3 (10-shot)Gemini (Ultra)87.8 (10-shot) PaLM-2 (Large) 86.8 (one shot)\\nStoryCloze GPT3 (175B)87.7 (few shot)PaLM-2 (Large)87.4 (one shot) OPT (175B) 79.82 (-)\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA PaLM-2 (Large)85.0 (one shot)LLaMa (65B)82.8 (zero shot) MT-NLG (530B)81.99 (zero shot)\\nTriviaQA PaLM-2 (Large)86.1 (one shot)LLaMA-2 (70B)85.0 (one shot) PaLM (540B) 81.4 (one shot)\\nContextual Language\\nUnderstanding LAMBADA PaLM (540B)89.7 (few shot)MT-NLG (530B)87.15 (few shot) PaLM-2 (Large) 86.9 (one shot)\\nCommonsense ReasoningWinoGrande GPT-4 (-) 87.5 (5-shot)PaLM-2 (Large)83.0 (one shot) PaLM (540B) 81.1 (zero shot)\\nSIQA LLaMA (65B)52.3 (zero shot)Chinchilla (70B)51.3 (zero shot) Gopher (280B) 50.6 (zero shot)\\nReading ComprehensionBoolQ PaLM(f) (540B) 92.2 (-) T5 (11B) 91.2 (-) PaLM-2 (Large) 90.9 (one shot)\\nTruthfulness Truthful-QA LLaMA (65B) 57 (-)\\nMathematical ReasoningMATH Gemini (Ultra)53.2 (4-shot)PaLM-2 (Large)34.3 (4-shot) LLaMa-2 (65B) 13.5 (4-shot)\\nGSM8K GPT-4 (-) 92.0 (5-shot)PaLM-2 (Large)80.7 (8-shot) U-PaLM (540B) 58.5 (-)\\nProblem Solving and\\nLogical Reasoning HumanEval Gemini(f) (Ultra) 74.4 (zero shot) GPT-4 (-) 67.0 (zero shot)Code Llama (34B)48.8 (zero shot)\\nin formulating new hypotheses and research questions since\\ntheir ability to process large-scale datasets allows them to un-\\nveil insights that might not be immediately apparent to human\\nresearchers [458]. Moreover, for scientific writing, LLMs can\\nhelp researchers draft documents, suggest improvements, and\\nensure adherence to specific formatting guidelines [459, 460].\\nThis not only saves time but also improves the clarity of scien-\\ntific communication, enabling interdisciplinary teams to work\\ntogether more effectively.\\nMaths: In addition to providing mathematical research and\\neducation support, LLMs can assist in solving mathematical\\nproblems by giving step-by-step explanations and guiding users\\nthrough complex proofs and calculations. They can help iden-\\ntify errors in reasoning or computation and suggest corrections,\\nserving as an invaluable tool for both learning and verification\\npurposes [461, 462]. LLMs can be employed to check the valid-\\nity of mathematical proofs, o ffering a preliminary filter before\\nhuman review. While they are not a substitute for the meticu-\\nlous work of mathematicians, they can help simplify the process\\nof proof verification [463, 464]. Moreover, LLMs enhance ac-\\ncessibility to mathematics by translating complex concepts and\\nfindings into understandable language for non-specialists [465],\\nwhere the gap between theoretical mathematics and applied\\ncontexts such as physics, engineering, and economics can be\\nbridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-\\numents, including generating initial coding for datasets, iden-\\ntifying themes, and classifying data according to these themes.\\nThis collaborative e ffort between legal experts and LLMs has\\nproved to be e ffective in analyzing legal texts such as court\\nopinions on theft, improving both the e fficiency and quality of\\nthe research [466]. Additionally, LLMs have been evaluated for\\ntheir ability to generate explanations of legal terms, focusing\\non improving factual accuracy and relevance by incorporating\\nsentences from case law. By feeding relevant case law into the\\nLLM, the augmented models can generate higher-quality expla-\\nnations with less factually incorrect information [467]. More-\\nover, LLMs can be trained with specialized domain knowledge\\nto perform legal reasoning tasks [468] and answer legal ques-\\ntions [469].\\nFinance: LLMs like BloombergGPT [151], trained on exten-\\nsive proprietary financial datasets, exhibit superior performance\\non financial tasks. This indicates the value of domain-specific\\ntraining in creating LLMs that can more accurately understand\\nand process industry-specific language and concepts. The intro-\\nduction of FinGPT [470] as an open-source model offers trans-\\nparent and accessible resources to develop novel applications\\nsuch as robo-advising, algorithmic trading, and low-code so-\\nlutions, ultimately expanding the capabilities of financial ser-\\nvices. Both BloombergGPT and FinGPT show the adaptabil-\\nity of LLMs to the financial domain, with the former showing\\nthe power of custom datasets and the latter emphasizing a data-\\ncentric approach and low-rank adaptation techniques for cus-\\ntomization. Moreover, LLMs demonstrate an ability to break\\ndown complex financial tasks into actionable plans, enabling\\nend-to-end solutions that were previously unfeasible with a sin-\\ngle model [471].\\nRobotics: In robotics research, LLMs have promising appli-\\ncations, such as enhancing human-robot interaction [28, 472,\\n473, 474], task planning [237], motion planning [246], nav-\\nigation [246, 475], object manipulation [236], personalized\\nrobots [476], etc. LLMs enable robots to understand the en-\\nvironment effectively and generate plans to complete tasks col-\\nlaboratively [240, 26]. They can facilitate continuous learning\\nby allowing robots to access and integrate information from a\\nwide range of sources, helping robots acquire new skills, adapt\\nto changes, and refine their paths [224, 233, 234].\\n7. Challenges and Future Directions\\nLLMs such as GPT-4 and its predecessors have significantly\\nadvanced natural language processing. Nevertheless, they also\\nbring along a set of challenges. The computational cost, ad-\\nversarial robustness, and interpretability are among the tech-\\nnical challenges that are intrinsic to these models. Further-\\nmore, as these models are scaled up to handle more complex\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 33, 'page_label': '34'}, page_content='tasks or to operate in more complex or dynamic environments,\\nnew challenges in scalability, privacy, and real-time processing\\nemerge. On the frontier of foundational research, integrating\\nmulti-modality and the effectiveness of transfer learning are be-\\ning keenly explored. Additionally, the continuous learning as-\\npect of these models, which aims to have models that can adapt\\nto new information over time, presents a fresh set of challenges.\\nThese challenges not only underscore the technical intricacies\\ninvolved but also highlight the broader impact and the future\\ntrajectory of LLMs in real-world applications. The following\\nsections delve into these challenges, shedding light on the on-\\ngoing and potential efforts to address them.\\nComputational Cost: Training LLMs require extensive compu-\\ntational resources, which increases production costs and raises\\nenvironmental concerns due to substantial energy consump-\\ntion during large-scale training. Improved performance occurs\\nas computational resources increase, but the rate of improve-\\nment gradually decreases when both the model and dataset\\nsize remain fixed, following the power law of diminishing re-\\nturns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-\\nases in their training data. These biases can manifest in the\\nmodel’s outputs, leading to potential ethical and fairness is-\\nsues [478].\\nOverfitting: Although LLMs possess substantial learning ca-\\npabilities, they are susceptible to overfitting noisy and peculiar\\npatterns within their extensive training data. Consequently, this\\nmay cause them to generate illogical responses [479]. The de-\\nbate about Memorization vs. Generalization in LLMs is about\\nfinding the right balance. Memorization allows the model to\\nremember specific details from its training data, ensuring it can\\nprovide accurate answers to precise questions. However, gen-\\neralization enables the model to make inferences and produce\\nresponses for inputs it has not seen before, which is essential\\nfor handling various real-world tasks. Striking the right bal-\\nance is the challenge: too much memorization can lead to over-\\nfitting, making the model inflexible and struggling with new\\ninputs [480].\\nEconomic and Research Inequality: The high cost of train-\\ning and deploying LLMs may make their development concen-\\ntrated within well-funded organizations, potentially worsening\\neconomic and research inequalities in AI [481].\\nReasoning and Planning: Some reasoning and planning tasks,\\neven as seemingly simple as common-sense planning, which\\nhumans find easy, remain well beyond the current capabilities\\nof LLMs evaluated using an assessment framework. This is not\\nentirely unexpected, considering that LLMs primarily generate\\ntext completions based on likelihood and offer no solid guaran-\\ntees in terms of reasoning abilities [482].\\nHallucinations: LLMs exhibit “hallucinations\", where they\\ngenerate responses that, while sounding plausible, are incorrect\\nor do not align with the provided information [483]. Hallucina-\\ntions can be categorized into three categories.\\n• Input-conflicting hallucination, wherein LLMs produce\\ncontent that diverges from the input given by users.\\n• Context-conflicting hallucination, where LLMs generate\\ncontent that contradicts information they have generated\\nearlier.\\n• Fact-conflicting hallucination involves LLM’s generation\\nof content that does not align with established world\\nknowledge.\\nPrompt Engineering: Prompts serve as inputs to LLMs, and\\ntheir syntax and semantics play a crucial role in determining\\nthe model’s output. The prompt variations, sometimes counter-\\nintuitive to humans, can result in significant changes in model\\noutput and are addressed through prompt engineering, which\\ninvolves designing natural language queries to guide LLMs\\nresponses effectively [484, 32].\\nLimited Knowledge: Information acquired during pretraining\\nis limited and may become obsolete after some time. Re-\\ntraining the model using updated data is costly. To generate\\nfactually accurate responses, people use a retrieval augmen-\\ntation pipeline [198]. However, pre-trained models are not\\ntrained with retrieval augmentation generation (RAG) [6, 21];\\nhence, adapting the training pipeline is necessary [193, 25].\\nSafety and Controllability: Using LLMs comes with the risk\\nof generating harmful, misleading, or inappropriate content,\\nwhether by accident or when given specific prompts. Ensuring\\nthese models are safely utilized is a significant concern [485].\\nSecurity and Privacy: LLMs are prone to leaking personal\\ninformation and generating false, unethical, misaligned re-\\nsponses. Researchers have explored various security attacks,\\ni.e., backdoor attacks, jailbreaking, prompt injection, and data\\npoisoning, that lead to breaking LLMs security. Therefore,\\ndeveloping better defense mechanisms is essential to ensure\\nLLMs are safe, reliable, and trustworthy for complex AI\\napplications [486].\\nMulti-Modality: Multi-modal learning, where LLMs are\\ntrained on diverse data like text, images, and videos, aims to\\ncreate models with richer understanding but faces challenges\\nin data alignment, fusion strategies, and higher computational\\ndemands.\\nCatastrophic Forgetting: LLMs are often pre-trained on\\nlarge datasets and then fine-tuned on domain-specific data,\\nreducing training resources. However, they face issues like\\ndomain adaptation and catastrophic forgetting, which hinder\\nthe retention of original knowledge when learning new tasks.\\nAdversarial Robustness: Large Language Models (LLMs)\\nhave shown great capabilities in various tasks but are vul-\\nnerable to adversarial attacks, where slight, deliberate input\\nalterations can mislead them. Especially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-\\nthough it sometimes compromises generalization [487]. As\\nLLMs integrate more into complex systems, examining their\\nsecurity properties becomes crucial, given the emerging field\\nof adversarial attacks on LLMs within trustworthy ML [488].\\nThis vulnerability is notable in safety-critical domains, ne-\\ncessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].\\nInterpretability and Explainability: The “black-box” nature\\nof LLMs poses challenges in understanding their decision-\\nmaking, which is crucial for broader acceptance and trust,\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 34, 'page_label': '35'}, page_content='especially in sensitive domains. Despite their advanced\\ncapabilities, the lack of insight into their operation limits their\\neffectiveness and trustworthiness [490, 491]. E fforts are being\\nmade to make LLMs more explainable to promote user trust\\nand to ensure responsible AI usage. Understanding the logic\\nbehind LLMs’ responses is essential for fostering trust and\\nensuring they align with human values and legal standards.\\nPrivacy Concerns: Privacy concerns in Large Language\\nModels (LLMs) have escalated with their growth in complexity\\nand size, particularly around data sharing and potential misuse.\\nThere is a risk of malicious content creation, filter bypass,\\nand data privacy issues, especially in e-commerce, where\\nprotecting customer privacy is crucial. If models are trained\\non private data, additional concerns arise if such models are\\nmade publicly available. LLMs tend to memorize phrases from\\ntheir training sets, which an adversary could exploit to extract\\nsensitive data, posing a threat to personal privacy [492, 493].\\nReal-Time Processing: Real-time processing in Large Lan-\\nguage Models (LLMs) is pivotal for various applications,\\nespecially with the rising popularity of mobile AI applications\\nand concerns regarding information security and privacy.\\nHowever, LLMs often have hundreds of layers and millions\\nof parameters, which impede real-time processing due to the\\nhigh computational demands and limited weight storage on\\nhardware platforms, particularly in edge computing environ-\\nments [494]. While certain e fforts like MobileBERT aim\\nto reduce memory requirements, they still face substantial\\nexecution overhead due to the large number of model layers,\\nleading to high inference latency.\\nLong-Term Dependencies: Large Language Models have\\nshown considerable progress in understanding and generating\\ntext, yet they often struggle with preserving context and\\nhandling long-term dependencies, particularly in complex,\\nmulti-turn conversations or long documents. This limitation\\ncan lead to incoherent or irrelevant responses.\\nHardware Acceleration: The growth of LLMs presents signif-\\nicant hardware challenges due to the increasing computational\\nand memory demands associated with training and deploying\\nthese models. GPUs have played a crucial role in meeting the\\nhardware requirements for training LLMs, with the networking\\nindustry also evolving to optimize hardware for training\\nworkloads. However, the growing size of LLMs, which has\\nbeen outpacing hardware progress, makes model inference in-\\ncreasingly costly. Model quantization is a promising approach\\nto bridge the widening gap between LLM size and hardware\\ncapacity [495]. Although specialized hardware acceleration\\nlike GPUs or TPUs can significantly reduce the computational\\ncost, making real-time applications more feasible, they may not\\nfully resolve all limitations, necessitating further advancements\\nin hardware technology.\\nRegulatory and Ethical Frameworks:The rapid advancements\\nin artificial intelligence have given rise to sophisticated Large\\nLanguage Models (LLMs) like OpenAI’s GPT-4 [157] and\\nGoogle’s Bard. These developments underscore the imperative\\nfor regulatory oversight to manage the ethical and social\\nchallenges accompanying LLMs’ widespread use [496]. For\\ninstance, LLMs can generate content that can be used posi-\\ntively or negatively, emphasizing the need for proactive ethical\\nframeworks and policy measures to guide their responsible\\nuse and assign accountability for their outputs [497]. Auditing\\nis identified as a promising governance mechanism to ensure\\nthat AI systems, including LLMs, are designed and deployed\\nethically, legally, and technically robust [498].\\n8. Conclusion\\nThis article has comprehensively reviewed the develop-\\nments in LLMs. It contributes to summarizing significant\\nfindings of LLMs in the existing literature and provides a\\ndetailed analysis of the design aspects, including architec-\\ntures, datasets, and training pipelines. We identified crucial\\narchitectural components and training strategies employed by\\ndifferent LLMs. These aspects are presented as summaries\\nand discussions throughout the article. Moreover, we have\\ndiscussed the performance di fferences of LLMs in zero-shot\\nand few-shot settings, explored the impact of fine-tuning, and\\ncompared supervised and generalized models and encoder vs.\\ndecoder vs. encoder-decoder architectures. A comprehensive\\nreview of multi-modal LLMs, retrieval augmented LLMs,\\nLLMs-powered agents, e fficient LLMs, datasets, evaluation,\\napplications, and challenges is also provided. This article is\\nanticipated to serve as a valuable resource for researchers,\\noffering insights into the recent advancements in LLMs and\\nproviding fundamental concepts and details to develop better\\nLLMs.\\nAcknowledgement: The author /s would like to acknowl-\\nedge the support received from Saudi Data and AI Authority\\n(SDAIA) and King Fahd University of Petroleum and Miner-\\nals (KFUPM) under SDAIA-KFUPM Joint Research Center for\\nArtificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\\ntory” for natural language processing?, in: Machine Learning and\\nKnowledge Discovery in Databases. Research Track: European Con-\\nference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\\nProceedings, Part III 21, Springer, 2021, pp. 677–693. 1\\n[2] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, S. Bowman, Superglue: A stickier benchmark for general-\\npurpose language understanding systems, Advances in neural informa-\\ntion processing systems 32 (2019). 1, 26, 29\\n[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\\nZ. Yang, A. Kulshreshtha, G. Nemade, Y . Lu, et al., Towards a human-\\nlike open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\\n[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n151 (2) (2022) 183–197. 2\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\\nLanguage models are unsupervised multitask learners, OpenAI blog\\n1 (8) (2019) 9. 2, 7\\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\\nare few-shot learners, Advances in neural information processing sys-\\ntems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018). 2, 18, 24\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 35, 'page_label': '36'}, page_content='[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nL. Zettlemoyer, Deep contextualized word representations, in: NAACL-\\nHLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\\n2\\n[9] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehen-\\nsion, arXiv preprint arXiv:1910.13461 (2019). 2\\n[10] C. Ra ffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\\na unified text-to-text transformer, The Journal of Machine Learning Re-\\nsearch 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31\\n[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, C. Ra ffel, mt5: A massively multilingual pre-trained text-to-\\ntext transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\\n25, 28, 30\\n[12] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi,\\nJ. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\\nguage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\\nparameter open-access multilingual language model, arXiv preprint\\narXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30\\n[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\\nM. Diab, X. Li, X. V . Lin, et al., Opt: Open pre-trained transformer\\nlanguage models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24,\\n25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\\ning language modeling with pathways, arXiv preprint arXiv:2204.02311\\n(2022). 2, 6, 9, 11, 23, 24, 25\\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\\nlanguage models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31\\n[17] V . Sanh, A. Webson, C. Ra ffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Cha ffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\\nprompted training enables zero-shot task generalization, arXiv preprint\\narXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31\\n[18] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\\nSuper-naturalinstructions: Generalization via declarative instructions on\\n1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\\n11, 16, 17, 24, 25, 28, 31\\n[19] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\\njishirzi, Self-instruct: Aligning language model with self generated in-\\nstructions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28\\n[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\\nels to follow instructions with human feedback, Advances in Neural In-\\nformation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\\n22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\\nfoundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\\n(2023). 2, 7, 10, 16, 25, 34\\n[22] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\\ngatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\\nlarge language models, arXiv preprint arXiv:2206.07682 (2022). 2\\n[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\\nlanguage models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\\n[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\\nentific research capabilities of large language models, arXiv preprint\\narXiv:2304.05332 (2023). 2\\n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\\nretrieval augmented language models, arXiv preprint arXiv:2208.03299\\n(2022). 2, 18, 19, 34\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33\\n[27] A. Parisi, Y . Zhao, N. Fiedel, Talm: Tool augmented language models,\\narXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models\\nfor human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\\n33\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y . Zhou, J. Wang, A. Hu, P. Shi,\\nY . Shi, et al., mplug-owl: Modularization empowers large language\\nmodels with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\\n22\\n[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\\nT. Lu, J. Zhou, Y . Qiao, et al., Visionllm: Large language model\\nis also an open-ended decoder for vision-centric tasks, arXiv preprint\\narXiv:2305.11175 (2023). 2, 22\\n[31] R. Yang, L. Song, Y . Li, S. Zhao, Y . Ge, X. Li, Y . Shan, Gpt4tools:\\nTeaching large language model to use tools via self-instruction, arXiv\\npreprint arXiv:2305.18752 (2023). 2, 19, 22, 23\\n[32] E. Saravia, Prompt Engineering Guide, https: //github.com/dair-\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\\nmodel, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25\\n[34] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5 +:\\nOpen code large language models for code understanding and genera-\\ntion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25\\n[35] S. Wang, Y . Sun, Y . Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\\nY . Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\\nedge enhanced pre-training for language understanding and generation,\\narXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25\\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y . He, Deepspeed: System op-\\ntimizations enable training deep learning models with over 100 billion\\nparameters, in: Proceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5\\n[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y . He, Zero: Memory optimiza-\\ntions toward training trillion parameter models, in: SC20: International\\nConference for High Performance Computing, Networking, Storage and\\nAnalysis, IEEE, 2020, pp. 1–16. 2, 4, 24\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\\na unified view of parameter-e fficient transfer learning, arXiv preprint\\narXiv:2110.04366 (2021). 2, 20, 21\\n[39] Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\\nria, Llm-adapters: An adapter family for parameter-e fficient fine-tuning\\nof large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\\n20\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\\nefficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\\nFrom dense to sparse: Contrastive pruning for better pre-trained lan-\\nguage model compression, in: Proceedings of the AAAI Conference on\\nArtificial Intelligence, V ol. 36, 2022, pp. 11547–11555. 2, 22\\n[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\\nAccurate and e fficient post-training quantization for large language\\nmodels, in: ICML, V ol. 202 of Proceedings of Machine Learning Re-\\nsearch, PMLR, 2023, pp. 38087–38099. 2, 21\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nCompression of generative pre-trained language models via quantiza-\\ntion, arXiv preprint arXiv:2203.10705 (2022). 2, 21\\n[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\\nGiraffe: Adventures in expanding context lengths in llms, arXiv preprint\\narXiv:2308.10882 (2023). 2, 17\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn: E fficient con-\\ntext window extension of large language models, arXiv preprint\\narXiv:2309.00071 (2023). 2, 17\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y .-H. Sung, Y . Yang,\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 36, 'page_label': '37'}, page_content='Longt5: E fficient text-to-text transformer for long sequences, arXiv\\npreprint arXiv:2112.07916 (2021). 2, 18\\n[49] S. Chen, S. Wong, L. Chen, Y . Tian, Extending context window\\nof large language models via positional interpolation, arXiv preprint\\narXiv:2306.15595 (2023). 2, 17\\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\\nJ. Zhang, Z. Dong, et al., A survey of large language models, arXiv\\npreprint arXiv:2303.18223 (2023). 2, 3, 7\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\\nvey on word representation models: From classical to state-of-the-art\\nword representation language models, Transactions on Asian and Low-\\nResource Language Information Processing 20 (5) (2021) 1–35. 2, 3\\n[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\\ncessing via large pre-trained language models: A survey, arXiv preprint\\narXiv:2111.01243 (2021). 2, 3\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He, et al., A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\\n2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\\narXiv:2301.00234 (2022). 2, 7, 18\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\\n[56] Y . Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\\nQ. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y . Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (V ol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-\\ning of the Association for Computational Linguistics (V olume 1: Long\\nPapers), 2016, pp. 1715–1725. 4\\n[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\\nIEEE international conference on acoustics, speech and signal process-\\ning (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Ra ffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y . Lee, B. Sagot, et al., Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization in\\nnlp, arXiv preprint arXiv:2112.10508 (2021). 4\\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017). 4, 7\\n[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\\nlinear biases enables input length extrapolation, in: International Con-\\nference on Learning Representations, 2022.\\nURL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\\n[66] J. Su, Y . Lu, S. Pan, A. Murtadha, B. Wen, Y . Liu, Roformer: En-\\nhanced transformer with rotary position embedding, arXiv preprint\\narXiv:2104.09864 (2021). 4, 9, 17\\n[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\\nwith sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\\n23\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness, Advances in Neural\\nInformation Processing Systems 35 (2022) 16344–16359. 4\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n[70] V . Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\\nmann machines, in: Proceedings of the 27th international conference on\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\npreprint arXiv:1606.08415 (2016). 4\\n[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overfitting, The\\njournal of machine learning research 15 (1) (2014) 1929–1958. 4\\n[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\\nKe, A. Goyal, Y . Bengio, A. Courville, C. Pal, Zoneout: Regular-\\nizing rnns by randomly preserving hidden activations, arXiv preprint\\narXiv:1606.01305 (2016). 4\\n[74] N. Shazeer, Glu variants improve transformer, arXiv preprint\\narXiv:2002.05202 (2020). 4\\n[75] Y . N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\\ngated convolutional networks, in: International conference on machine\\nlearning, PMLR, 2017, pp. 933–941. 4\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\narXiv:1607.06450 (2016). 4\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\nin Neural Information Processing Systems 32 (2019). 4\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\\ntransformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\\n[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\\nMegatron-lm: Training multi-billion parameter language models using\\nmodel parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\\n[81] \"bmtrain: E fficient training for big models.\".\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n[82] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\\ntac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\\nart natural language processing, in: Proceedings of the 2020 conference\\non empirical methods in natural language processing: system demon-\\nstrations, 2020, pp. 38–45. 5\\n[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\\nrin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\\nJax: composable transformations of python + numpy programs (2018).\\n5\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y . Liu, H. Huang, B. Wang, Y . You,\\nColossal-ai: A unified deep learning system for large-scale parallel train-\\ning, arXiv preprint arXiv:2110.14883 (2021). 5\\n[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe: A\\nfast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\\n(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\\nwork, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\\n162. 5\\n[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\\native style, high-performance deep learning library, Advances in neural\\ninformation processing systems 32 (2019). 5\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\\nscale machine learning., in: Osdi, V ol. 16, Savannah, GA, USA, 2016,\\npp. 265–283. 5\\n[89] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao,\\nB. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and e fficient machine\\nlearning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\\nlion parameter models with simple and efficient sparsity, The Journal of\\nMachine Learning Research 23 (1) (2022) 5232–5270. 5, 9\\n[91] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\\nmodels with mixture-of-experts, in: International Conference on Ma-\\nchine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25\\n[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y . Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu- P: Towards trillion\\nparameter language model with sparse heterogeneous computing, arXiv\\npreprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, C. Ra ffel, What language model architecture and pretrain-\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 37, 'page_label': '38'}, page_content='ing objective works best for zero-shot generalization?, in: International\\nConference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou,\\nH.-W. Hon, Unified language model pre-training for natural language\\nunderstanding and generation, Advances in neural information process-\\ning systems 32 (2019). 6\\n[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\\nS. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\\nmodels, arXiv preprint arXiv:2001.08361 (2020). 6\\n[96] J. Ho ffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\\net al., Training compute-optimal large language models, arXiv preprint\\narXiv:2203.15556 (2022). 6, 9, 25, 29\\n[97] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\\nT. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\\nstruction meta learning through the lens of generalization, arXiv preprint\\narXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\\n[98] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang, C. Gan,\\nPrinciple-driven self-alignment of language models from scratch with\\nminimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\\n7, 17\\n[99] A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\\nN. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\\nas a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\\n7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\\nP. Christiano, G. Irving, Fine-tuning language models from human pref-\\nerences, arXiv preprint arXiv:1909.08593 (2019). 7\\n[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\\ntion: Improving zero-shot and few-shot learning of language models via\\nchain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\\nining the power of symbolic tasks in instruction tuning, arXiv preprint\\narXiv:2304.07995 (2023). 7, 16\\n[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\\nD. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\\nlanguage models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23\\n[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022). 7, 20\\n[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, K. Narasimhan,\\nTree of thoughts: Deliberate problem solving with large language mod-\\nels, arXiv preprint arXiv:2305.10601 (2023). 7, 20\\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\\ning for nlp, in: International Conference on Machine Learning, PMLR,\\n2019, pp. 2790–2799. 7, 20\\n[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\\nof large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\\n[108] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang, et al., Pangu- α : Large-scale autoregressive pre-\\ntrained chinese language models with auto-parallel computation, arXiv\\npreprint arXiv:2104.12369 (2021). 8, 23, 24, 25\\n[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y . Cen, X. Zou, Z. Yang,\\nJ. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\\ntraining language models, AI Open 2 (2021) 65–68. 8, 30\\n[110] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation, arXiv preprint\\narXiv:2107.02137 (2021). 8, 25\\n[111] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, R. Salakhutdinov,\\nTransformer-xl: Attentive language models beyond a fixed-length con-\\ntext, arXiv preprint arXiv:1901.02860 (2019). 8\\n[112] O. Lieber, O. Sharir, B. Lenz, Y . Shoham, Jurassic-1: Technical details\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\\n[113] Y . Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\\nficiencies of self-attention, Advances in Neural Information Processing\\nSystems 33 (2020) 22640–22651. 8, 11\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\\nmodels bring? intensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25\\n[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\\nL. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\\nshot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\\n24, 25\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Ho ffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\\nguage models: Methods, analysis & insights from training gopher, arXiv\\npreprint arXiv:2112.11446 (2021). 8, 9, 25, 28\\n[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti, et al.,\\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a\\nlarge-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25\\n[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\\nH. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\\nsource autoregressive language model, arXiv preprint arXiv:2204.06745\\n(2022). 9, 23, 24, 25\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9\\n[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\\ncision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\\n[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\\nton, J. Dean, Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\\n[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\\natm 20b: Few-shot learning using a large-scale multilingual seq2seq\\nmodel, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25\\n[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\\narXiv preprint arXiv:2305.10403 (2023). 9, 25\\n[124] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Garcia,\\nH. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\\nwith 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\\n24, 25\\n[125] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\\nguage learning paradigms, in: The Eleventh International Conference\\non Learning Representations, 2022. 9, 10, 24, 25\\n[126] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\\neral language model pretraining with autoregressive blank infilling, in:\\nProceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (V olume 1: Long Papers), 2022, pp. 320–335. 10\\n[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and e fficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023). 10, 23, 25\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\npreprint arXiv:2112.05682 (2021). 10\\n[129] V . A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\nM. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\\ntransformer models, Proceedings of Machine Learning and Systems 5\\n(2023). 10\\n[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of\\nmodels, arXiv preprint arXiv:2407.21783 (2024). 10, 25\\n[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25\\n[132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n25\\n[133] https://github.com/xai-org/grok-1. 10\\n[134] https://x.ai/blog/grok-1.5. 10\\n[135] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly\\ncapable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10\\n[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 38, 'page_label': '39'}, page_content='Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem-\\nini 1.5: Unlocking multimodal understanding across millions of tokens\\nof context, arXiv preprint arXiv:2403.05530 (2024). 10\\n[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun-\\ndyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b\\ntechnical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25\\n[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\\nQ. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\\nwith longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25\\n[139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,\\nC. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li,\\nF. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang,\\nH. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\\nJ. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao,\\nK. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li,\\nM. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang,\\nP. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge,\\nR. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye,\\nS. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\\nT. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao,\\nW. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen,\\nX. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical,\\nand efficient mixture-of-experts language model, CoRR abs/2405.04434\\n(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,\\nC. Xiong, Codegen: An open large language model for code with multi-\\nturn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11,\\n23, 25, 28\\n[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\\nwards, Y . Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31\\n[142] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\\ncode generation with alphacode, Science 378 (6624) (2022) 1092–1097.\\n11, 23, 25, 29\\n[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\narXiv preprint arXiv:1911.02150 (2019). 11\\n[144] R. Y . Pang, H. He, Text generation by learning from demonstrations,\\narXiv preprint arXiv:2009.07839 (2020). 11\\n[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine\\ntranslation models, arXiv preprint arXiv:2009.09372 (2020). 11\\n[146] Y . Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and genera-\\ntion, arXiv preprint arXiv:2109.00859 (2021). 11\\n[147] R. Li, L. B. Allal, Y . Zi, N. Muennigho ff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\\nwith you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25\\n[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\\nA. Poulton, V . Kerkez, R. Stojnic, Galactica: A large language model for\\nscience, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\\n[149] FairScale authors, Fairscale: A general purpose modular pytorch library\\nfor high performance and large scale training, https://github.com/\\nfacebookresearch/fairscale (2021). 11\\n[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y . Du, et al., Lamda: Language models\\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\\n[151] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33\\n[152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\\ncial chat model with hundreds of billions parameters, arXiv preprint\\narXiv:2305.12002 (2023). 11, 17, 25\\n[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\nformer language model with jax (2021). 12, 24\\n[154] N. Muennigho ff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\\nCrosslingual generalization through multitask finetuning, arXiv preprint\\narXiv:2211.01786 (2022). 16, 25, 28, 31\\n[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\\nDynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue, et al., Llama-adapter v2: Parameter-e fficient visual in-\\nstruction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\\n[157] Openai. gpt-4 technical report (2023). 16, 35\\n[158] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nT. B. Hashimoto, Stanford alpaca: An instruction-following llama\\nmodel, https://github.com/tatsu-lab/stanford_alpaca\\n(2023). 16, 25, 28\\n[159] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng,\\nS. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).\\nURL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\\n28\\n[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprint\\narXiv:2304.06975 (2023). 16\\n[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\\nWizardlm: Empowering large language models to follow complex in-\\nstructions, arXiv preprint arXiv:2304.12244 (2023). 16\\n[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nD. Jiang, Wizardcoder: Empowering code large language models with\\nevol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\\n[165] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\\ning language models to support answers with verified quotes, arXiv\\npreprint arXiv:2203.11147 (2022). 17\\n[166] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders, et al., Webgpt: Browser-\\nassisted question-answering with human feedback, arXiv preprint\\narXiv:2112.09332 (2021). 17, 19, 20, 25, 31\\n[167] A. Glaese, N. McAleese, M. Tr˛ ebacz, J. Aslanides, V . Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\\nalignment of dialogue agents via targeted human judgements, arXiv\\npreprint arXiv:2209.14375 (2022). 17, 20, 25\\n[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\\nDirect preference optimization: Your language model is secretly a re-\\nward model, arXiv preprint arXiv:2305.18290 (2023). 17\\n[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\\nT. Zhang, Raft: Reward ranked finetuning for generative foundation\\nmodel alignment, arXiv preprint arXiv:2304.06767 (2023). 17\\n[170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\\nresponses to align language models with human feedback without tears,\\narXiv preprint arXiv:2304.05302 (2023). 17\\n[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y . Li, H. Wang, Preference rank-\\ning optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17\\n[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\\ntuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\\n17\\n[173] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\\nA. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\\nlessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\\n[174] Y . Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang, T. B. Hashimoto, Alpacafarm: A simulation frame-\\nwork for methods that learn from human feedback, arXiv preprint\\narXiv:2305.14387 (2023). 17\\n[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\\nPrompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17\\n[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši ¯ut˙e, A. Chen,\\nA. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\\nity for moral self-correction in large language models, arXiv preprint\\narXiv:2302.07459 (2023). 17\\n[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 17\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 39, 'page_label': '40'}, page_content='[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y . Bai, S. Kadavath,\\nB. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\\nguage models to reduce harms: Methods, scaling behaviors, and lessons\\nlearned, arXiv preprint arXiv:2209.07858 (2022). 17, 28\\n[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\\nlish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17\\n[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\\nN. McAleese, G. Irving, Red teaming language models with language\\nmodels, arXiv preprint arXiv:2202.03286 (2022). 17\\n[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\\ncontinual learners, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 6107–6122. 17\\n[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\\nC. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17\\n[184] H. Chen, Y . Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y . Yanggong,\\nJ. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\\nof low training data instruction tuning, arXiv preprint arXiv:2305.09246\\n(2023). 17\\n[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat,\\nP. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\\narXiv:2305.11206 (2023). 17, 25, 28\\n[186] C. Han, Q. Wang, W. Xiong, Y . Chen, H. Ji, S. Wang, Lm-infinite: Sim-\\nple on-the-fly length generalization for large language models, arXiv\\npreprint arXiv:2308.16137 (2023). 17, 18\\n[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y . Zemlyan-\\nskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y . Tay, et al., Colt5: Faster\\nlong-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18\\n[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\\nLongnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\\narXiv:2307.02486 (2023). 18\\n[189] Y . Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\\ncient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18\\n[190] N. Ratner, Y . Levine, Y . Belinkov, O. Ram, I. Magar, O. Abend,\\nE. Karpas, A. Shashua, K. Leyton-Brown, Y . Shoham, Parallel context\\nwindows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18\\n[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\\nReflexion: Language agents with verbal reinforcement learning, arXiv\\npreprint arXiv:2303.11366 14 (2023). 18, 20\\n[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\\ning llms with databases as their symbolic memory, arXiv preprint\\narXiv:2306.03901 (2023). 18\\n[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, G. Neubig, Active retrieval augmented generation, arXiv\\npreprint arXiv:2305.06983 (2023). 18\\n[198] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, Y . Shoham, In-context retrieval-augmented language models,\\narXiv preprint arXiv:2302.00083 (2023). 18, 34\\n[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18\\n[200] D. Schuurmans, Memory augmented large language models are compu-\\ntationally universal, arXiv preprint arXiv:2301.04589 (2023). 18\\n[201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\\ngeneral read-write memory for large language models, arXiv preprint\\narXiv:2305.14322 (2023). 18\\n[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\\nwork: Bm25 and beyond, Foundations and Trends® in Information Re-\\ntrieval 3 (4) (2009) 333–389. 18\\n[203] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, D. Zhou,\\nRationale-augmented ensembles in language models, arXiv preprint\\narXiv:2207.00747 (2022). 18\\n[204] F. Zhang, B. Chen, Y . Zhang, J. Liu, D. Zan, Y . Mao, J.-G. Lou, W. Chen,\\nRepocoder: Repository-level code completion through iterative retrieval\\nand generation, arXiv preprint arXiv:2303.12570 (2023). 18\\n[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\\nO. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study, arXiv preprint\\narXiv:2304.06762 (2023). 19\\n[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19\\n[207] J. Liu, D. Shen, Y . Zhang, B. Dolan, L. Carin, W. Chen, What makes\\ngood in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\\n(2021). 19\\n[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19\\n[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\\nmodels, arXiv preprint arXiv:2301.12652 (2023). 19\\n[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\narXiv preprint arXiv:2306.13421 (2023). 19\\n[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\\nlanguage model pre-training, in: International conference on machine\\nlearning, PMLR, 2020, pp. 3929–3938. 19\\n[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: E fficient and ef-\\nfective retrieval-augmented text generation, in: Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2023, pp. 1437–1447. 19\\n[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\nation, arXiv preprint arXiv:2107.07566 (2021). 19\\n[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\\naugmented language models through few-shot prompting for open-\\ndomain question answering, arXiv preprint arXiv:2203.05115 (2022).\\n19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\\ngpt: A general multi-modal assistant that can plan, execute, inspect, and\\nlearn, arXiv preprint arXiv:2306.08640 (2023). 19\\n[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y . N. Wu, S.-C. Zhu,\\nJ. Gao, Chameleon: Plug-and-play compositional reasoning with large\\nlanguage models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23\\n[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\\nRibeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\\nguage models, arXiv preprint arXiv:2303.09014 (2023). 19\\n[218] C.-Y . Hsieh, S.-A. Chen, C.-L. Li, Y . Fujii, A. Ratner, C.-Y . Lee, R. Kr-\\nishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\\nlarge language models, arXiv preprint arXiv:2308.00675 (2023). 19\\n[219] Y . Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y . Tian, S. Li, Restgpt:\\nConnecting large language models with real-world applications via rest-\\nful apis, arXiv preprint arXiv:2306.06624 (2023). 19\\n[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\\nguage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 40, 'page_label': '41'}, page_content='20\\n[224] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, Y . Zhuang, Hugginggpt: Solv-\\ning ai tasks with chatgpt and its friends in huggingface, arXiv preprint\\narXiv:2303.17580 (2023). 19, 20, 33\\n[225] Y . Liang, C. Wu, T. Song, W. Wu, Y . Xia, Y . Liu, Y . Ou, S. Lu, L. Ji,\\nS. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\\ndation models with millions of apis, arXiv preprint arXiv:2303.16434\\n(2023). 19\\n[226] D. Surís, S. Menon, C. V ondrick, Vipergpt: Visual inference via python\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\\n[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\\nuser assistance systems, Business & Information Systems Engineering\\n58 (2016) 367–370. 20\\n[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\\nmulti-agent collaborative framework, arXiv preprint arXiv:2308.00352\\n(2023). 20\\n[230] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou, et al., The rise and potential of large language model\\nbased agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\\n[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\\nX. Chen, Y . Lin, et al., A survey on large language model based au-\\ntonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20\\n[232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\\nshot planners: Extracting actionable knowledge for embodied agents,\\nin: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20\\n[233] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\\ning with language model is planning with world model, arXiv preprint\\narXiv:2305.14992 (2023). 20, 33\\n[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y . Feng, L. Xue, R. Murthy,\\nZ. Chen, J. Zhang, D. Arpit, et al., Retroformer: Retrospective\\nlarge language agents with policy gradient optimization, arXiv preprint\\narXiv:2308.02151 (2023). 20, 33\\n[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\\nlogue: Embodied reasoning through planning with language models, in:\\n6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20\\n[236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\\nEmbodied finetuning for vision-language reasoning in robot manipula-\\ntion, arXiv preprint arXiv:2305.18898 (2023). 20, 33\\n[237] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, A. Garg, Progprompt: Generating situated robot task plans\\nusing large language models, in: 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20,\\n33\\n[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\\nChiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\\nfor robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\\n[239] X. Tang, A. Zou, Z. Zhang, Y . Zhao, X. Zhang, A. Cohan, M. Gerstein,\\nMedagents: Large language models as collaborators for zero-shot med-\\nical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20\\n[240] A. Brohan, Y . Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\\nJ. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\\nGrounding language in robotic a ffordances, in: Conference on Robot\\nLearning, PMLR, 2023, pp. 287–318. 20, 33\\n[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\\nguided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\\n20\\n[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\\nnav: Grounding large language models for dynamic planning to navi-\\ngation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20\\n[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y . Su,\\nLlm-planner: Few-shot grounded planning for embodied agents with\\nlarge language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n[244] V . S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\\npreprint arXiv:2303.03480 (2023). 20\\n[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\\nrobot navigation, in: 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\\n[246] Y . Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\\nwith large language models for object rearrangement, arXiv preprint\\narXiv:2303.06247 (2023). 20, 33\\n[247] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, J. Tang, Gpt under-\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\\n[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-e fficient tun-\\ning: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\\n20\\n[249] Y . Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\\nAdamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\\nguage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\\n[250] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nW. Chen, Lora: Low-rank adaptation of large language models, arXiv\\npreprint arXiv:2106.09685 (2021). 21, 22, 23\\n[251] X. Liu, K. Ji, Y . Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks, in: Pro-\\nceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (V olume 2: Short Papers), 2022, pp. 61–68. 21\\n[252] A. Razdaibiedina, Y . Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\\nProgressive prompts: Continual learning for language models, arXiv\\npreprint arXiv:2301.12314 (2023). 21\\n[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\\nwards adaptive prefix tuning for parameter-e fficient language model\\nfine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\\n[254] E. B. Zaken, S. Ravfogel, Y . Goldberg, Bitfit: Simple parameter-\\nefficient fine-tuning for transformer-based masked language-models,\\narXiv preprint arXiv:2106.10199 (2021). 21\\n[255] T. Dettmers, M. Lewis, Y . Belkada, L. Zettlemoyer, Llm. int8 ():\\n8-bit matrix multiplication for transformers at scale, arXiv preprint\\narXiv:2208.07339 (2022). 21, 22\\n[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: Accurate\\npost-training quantization for generative pre-trained transformers, arXiv\\npreprint arXiv:2210.17323 (2022). 21\\n[257] X. Wei, Y . Zhang, Y . Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\\npression+: Accurate quantization of large language models by equiva-\\nlent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21\\n[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\\naccurate post-training quantization and pruning, Advances in Neural In-\\nformation Processing Systems 35 (2022) 4475–4488. 21\\n[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, arXiv\\npreprint arXiv:2306.02272 (2023). 21\\n[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\\nW. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained language models, arXiv\\npreprint arXiv:2210.03858 (2022). 21\\n[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: E fficient\\nfinetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22\\n[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-\\nishnamoorthi, V . Chandra, Llm-qat: Data-free quantization aware train-\\ning for large language models, arXiv preprint arXiv:2305.17888 (2023).\\n21, 22\\n[263] Y . Guo, A. Yao, H. Zhao, Y . Chen, Network sketching: Exploiting bi-\\nnary structure in deep cnns, in: Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\\nMemory-efficient fine-tuning of compressed large language models via\\nsub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\\n22\\n[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and e ffective pruning\\napproach for large language models, arXiv preprint arXiv:2306.11695\\n(2023). 22\\n[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 41, 'page_label': '42'}, page_content='[267] L. Yin, Y . Wu, Z. Zhang, C.-Y . Hsieh, Y . Wang, Y . Jia, M. Pechenizkiy,\\nY . Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\\nmissing secret sauce for pruning llms to high sparsity, arXiv preprint\\narXiv:2310.05175 (2023). 22\\n[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nStructured pruning for efficient generative pre-trained language models,\\nin: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22\\n[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\\nguage model for few-shot learning, Advances in Neural Information Pro-\\ncessing Systems 35 (2022) 23716–23736. 22\\n[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models,\\narXiv preprint arXiv:2301.12597 (2023). 22\\n[271] H. Liu, C. Li, Q. Wu, Y . J. Lee, Visual instruction tuning, arXiv preprint\\narXiv:2304.08485 (2023). 22\\n[272] K. Li, Y . He, Y . Wang, Y . Li, W. Wang, P. Luo, Y . Wang, L. Wang,\\nY . Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22\\n[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\\ntailed video understanding via large vision and language models, arXiv\\npreprint arXiv:2306.05424 (2023). 22\\n[274] H. Zhang, X. Li, L. Bing, Video-llama: An instruction-tuned\\naudio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22\\n[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\\nY . Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\\ndio captioning dataset for audio-language multimodal research, arXiv\\npreprint arXiv:2303.17395 (2023). 22\\n[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\\nllm: Multi-modal language modeling with image, audio, video, and text\\nintegration, arXiv preprint arXiv:2306.09093 (2023). 22\\n[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models,\\narXiv preprint arXiv:2304.10592 (2023). 22\\n[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020). 22\\n[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\\nS. Hoi, Instructblip: Towards general-purpose vision-language models\\nwith instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\\n[280] Z. Xu, Y . Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\\nshot learning via instruction tuning, arXiv preprint arXiv:2212.10773\\n(2022). 22\\n[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\\nChatbridge: Bridging modalities with large language model as a lan-\\nguage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\\n[282] L. Li, Y . Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y . Yang, J. Xu,\\nX. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\\nlingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\\n[283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\\nH. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\\narXiv preprint arXiv:2305.14167 (2023). 22\\n[284] G. Luo, Y . Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\\nEfficient vision-language instruction tuning for large language models,\\narXiv preprint arXiv:2305.15023 (2023). 22\\n[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y . Qiao,\\nLlama-adapter: E fficient fine-tuning of language models with zero-init\\nattention, arXiv preprint arXiv:2303.16199 (2023). 22\\n[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\\nRobust speech recognition via large-scale weak supervision, in: Inter-\\nnational Conference on Machine Learning, PMLR, 2023, pp. 28492–\\n28518. 22\\n[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\\nmodal chain-of-thought reasoning in language models, arXiv preprint\\narXiv:2302.00923 (2023). 23\\n[288] J. Ge, H. Luo, S. Qian, Y . Gan, J. Fu, S. Zhan, Chain of thought prompt\\ntuning in vision language models, arXiv preprint arXiv:2304.07919\\n(2023). 23\\n[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\\ning, drawing and editing with visual foundation models, arXiv preprint\\narXiv:2303.04671 (2023). 23\\n[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\\nM. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\\nsoning and action, arXiv preprint arXiv:2303.11381 (2023). 23\\n[291] T. Wang, J. Zhang, J. Fei, Y . Ge, H. Zheng, Y . Tang, Z. Li, M. Gao,\\nS. Zhao, Y . Shan, et al., Caption anything: Interactive image descrip-\\ntion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\\n(2023). 23\\n[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\\nAdapting clip for powerful 3d open-world learning, arXiv preprint\\narXiv:2211.11682 (2022). 23\\n[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\\nsoning without training, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\\n23\\n[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\\nfusion with intra-and inter-modality attention flow for visual question\\nanswering, in: Proceedings of the IEEE /CVF conference on computer\\nvision and pattern recognition, 2019, pp. 6639–6648. 23\\n[295] Z. Yu, J. Yu, Y . Cui, D. Tao, Q. Tian, Deep modular co-attention net-\\nworks for visual question answering, in: Proceedings of the IEEE /CVF\\nconference on computer vision and pattern recognition, 2019, pp. 6281–\\n6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\\nW. Chang, S.-F. Chang, Idealgpt: Iteratively decomposing vision\\nand language reasoning via large language models, arXiv preprint\\narXiv:2305.14985 (2023). 23\\n[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y . Qiao, P. Gao, H. Li,\\nPrompt, generate, then cache: Cascade of foundation models makes\\nstrong few-shot learners, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\\n23\\n[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\\nnormalization of self-attention, CoRR abs/1910.05895 (2019). 24\\n[299] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, V . Stoyanov, Roberta: A robustly optimized bert pre-\\ntraining approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\\n[300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nD. Song, Koala: A dialogue model for academic research, Blog post\\n(April 2023).\\nURL https://bair.berkeley.edu/blog/2023/04/03/koala/\\n25\\n[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile: An\\n800gb dataset of diverse text for language modeling, arXiv preprint\\narXiv:2101.00027 (2020). 28, 30\\n[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. V on Werra, C. Mou, E. González Ponferrada, H. Nguyen,\\net al., The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset, Advances in Neural Information Processing Systems 35 (2022)\\n31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n[304] Together Computer, Redpajama: An open source recipe to reproduce\\nllama training dataset (Apr. 2023).\\nURL https://github.com/togethercomputer/\\nRedPajama-Data 28\\n[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\\nTuning language models with (almost) no human labor, arXiv preprint\\narXiv:2212.09689 (2022). 28\\n[306] Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\\nD. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\\nharmless assistant with reinforcement learning from human feedback,\\narXiv preprint arXiv:2204.05862 (2022). 28\\n[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\\nJ. Steinhardt, Measuring massive multitask language understanding,\\narXiv preprint arXiv:2009.03300 (2020). 26, 29\\n[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 42, 'page_label': '43'}, page_content='the imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models, arXiv preprint arXiv:2206.04615 (2022). 26, 29\\n[309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\\nA multi-task benchmark and analysis platform for natural language un-\\nderstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29\\n[310] Y . Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\\neration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\\n29\\n[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y . Li, Y . Xu, K. Sun, D. Yu,\\nC. Yu, et al., Clue: A chinese language understanding evaluation bench-\\nmark, arXiv preprint arXiv:2004.05986 (2020). 29\\n[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\\nbenchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y .-L. Boureau, Can\\nyou put it all together: Evaluating conversational agents’ ability to blend\\nskills, arXiv preprint arXiv:2004.08449 (2020). 29\\n[314] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY . Zhang, D. Narayanan, Y . Wu, A. Kumar, et al., Holistic evaluation of\\nlanguage models, arXiv preprint arXiv:2211.09110 (2022). 29\\n[315] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\\nY . Song, T. Oh, et al., Klue: Korean language understanding evaluation,\\narXiv preprint arXiv:2105.09680 (2021). 29\\n[316] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\\nanswering challenge, Transactions of the Association for Computational\\nLinguistics 7 (2019) 249–266. 27, 29\\n[317] M. T. Pilehvar, J. Camacho-Collados, Wic: 10,000 example\\npairs for evaluating context-sensitive representations, arXiv preprint\\narXiv:1808.09121 6 (2018). 27, 29\\n[318] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\\nmodels, arXiv preprint arXiv:1609.07843 (2016). 28, 29\\n[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\\nsive transformers for long-range sequence modelling, arXiv preprint\\narXiv:1911.05507 (2019). 28, 29\\n[320] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\\nlarge-scale chinese question matching corpus, in: Proceedings of the\\n27th international conference on computational linguistics, 2018, pp.\\n1952–1962. 28, 29\\n[321] S. Iyer, N. Dandekar, K. Csernai, First quora dataset re-\\nlease: Question pairs, https://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs . 29\\n[322] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\\ncoreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\\n[323] M.-C. De Marne ffe, M. Simons, J. Tonhauser, The commitmentbank: In-\\nvestigating projection in naturally occurring discourse, in: proceedings\\nof Sinn und Bedeutung, V ol. 23, 2019, pp. 107–124. 29\\n[324] Z. Li, N. Ding, Z. Liu, H. Zheng, Y . Shen, Chinese relation extraction\\nwith multi-grained information and external linguistic knowledge, in:\\nProceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics, 2019, pp. 4377–4386. 29\\n[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\\nand relation extraction dataset for chinese literature text, arXiv preprint\\narXiv:1711.07010 (2017). 29\\n[326] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\\nlarge-scale domain-specific chinese corpus for sentence semantic equiv-\\nalence identification, in: Proceedings of the 2018 conference on empiri-\\ncal methods in natural language processing, 2018, pp. 4946–4951. 29\\n[327] B. Liu, D. Niu, H. Wei, J. Lin, Y . He, K. Lai, Y . Xu, Matching arti-\\ncle pairs with graphical decomposition and convolutions, arXiv preprint\\narXiv:1802.07459 (2018). 29\\n[328] P. Li, W. Li, Z. He, X. Wang, Y . Cao, J. Zhou, W. Xu, Dataset and neu-\\nral recurrent sequence labeling model for open-domain factoid question\\nanswering, arXiv preprint arXiv:1607.06275 (2016). 29\\n[329] N. Peng, M. Dredze, Named entity recognition for chinese social media\\nwith jointly trained embeddings, in: Proceedings of the 2015 conference\\non empirical methods in natural language processing, 2015, pp. 548–\\n554. 29\\n[330] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\\nnale generation: Learning to solve and explain algebraic word problems,\\narXiv preprint arXiv:1705.04146 (2017). 29\\n[331] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\\nlease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\\ntium (2011). 29\\n[332] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\\ncomplex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\\n[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\\nin social media: A case study of african-american english, arXiv preprint\\narXiv:1608.08868 (2016). 29\\n[334] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, J. Allen, A corpus and evaluation framework\\nfor deeper understanding of commonsense stories, arXiv preprint\\narXiv:1604.01696 (2016). 28, 29\\n[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\\nWord prediction requiring a broad discourse context, arXiv preprint\\narXiv:1606.06031 (2016). 28, 29\\n[336] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\\nrization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\\n[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\\nation with planning-based hierarchical variational model, arXiv preprint\\narXiv:1908.06605 (2019). 29\\n[338] J. Novikova, O. Dušek, V . Rieser, The e2e dataset: New challenges for\\nend-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\\n[339] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\\nfor cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\\n[340] Y . Bisk, R. Zellers, J. Gao, Y . Choi, et al., Piqa: Reasoning about phys-\\nical commonsense in natural language, in: Proceedings of the AAAI\\nconference on artificial intelligence, V ol. 34, 2020, pp. 7432–7439. 28,\\n29\\n[341] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\\ndistantly supervised challenge dataset for reading comprehension, arXiv\\npreprint arXiv:1705.03551 (2017). 28, 29, 31\\n[342] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nO. Tafjord, Think you have solved question answering? try arc, the ai2\\nreasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 28, 29,\\n31\\n[343] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost: Phys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29\\n[344] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering, arXiv\\npreprint arXiv:1809.02789 (2018). 29\\n[345] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\\nwebnlg+ shared task overview and evaluation results (webnlg + 2020),\\nin: Proceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG+), 2020. 29\\n[346] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\\nA chinese dataset for cant understanding with common sense and world\\nknowledge, arXiv preprint arXiv:2104.02704 (2021). 29\\n[347] G. Lai, Q. Xie, H. Liu, Y . Yang, E. Hovy, Race: Large-scale\\nreading comprehension dataset from examinations, arXiv preprint\\narXiv:1704.04683 (2017). 29\\n[348] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang,\\nL. Zettlemoyer, Quac: Question answering in context, arXiv preprint\\narXiv:1808.07036 (2018). 29\\n[349] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\\ntle use a laptop? a question answering benchmark with implicit reason-\\ning strategies, Transactions of the Association for Computational Lin-\\nguistics 9 (2021) 346–361. 29, 31\\n[350] J. Boyd-Graber, B. Satino ff, H. He, H. Daumé III, Besting the quiz mas-\\nter: Crowdsourcing incremental classification games, in: Proceedings of\\nthe 2012 joint conference on empirical methods in natural language pro-\\ncessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29\\n[351] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\\nquestion answer matching using end-to-end character-level multi-scale\\ncnns, Applied Sciences 7 (8) (2017) 767. 29\\n[352] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\\nteraction networks for chinese medical question answer selection, IEEE\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 43, 'page_label': '44'}, page_content='Access 6 (2018) 74061–74071. 29\\n[353] C. Xu, J. Pei, H. Wu, Y . Liu, C. Li, Matinf: A jointly labeled large-scale\\ndataset for classification, question answering and summarization, arXiv\\npreprint arXiv:2004.12302 (2020). 29\\n[354] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y . Choi, Winogrande: An\\nadversarial winograd schema challenge at scale, Communications of the\\nACM 64 (9) (2021) 99–106. 27, 29\\n[355] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, Y . Choi, Hellaswag: Can a\\nmachine really finish your sentence?, arXiv preprint arXiv:1905.07830\\n(2019). 29\\n[356] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\\nnatives: An evaluation of commonsense causal reasoning., in: AAAI\\nspring symposium: logical formalizations of commonsense reasoning,\\n2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\\nlenge, in: Thirteenth international conference on the principles of knowl-\\nedge representation and reasoning, 2012. 27, 29\\n[358] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, arXiv preprint\\narXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y . Choi, Socialiqa:\\nCommonsense reasoning about social interactions, arXiv preprint\\narXiv:1904.09728 (2019). 29\\n[360] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\\nlenging chinese machine reading comprehension, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 141–155. 29\\n[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\\ning the gap between human and machine commonsense reading compre-\\nhension, arXiv preprint arXiv:1810.12885 (2018). 29\\n[362] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\\nfor machine comprehension of text, arXiv preprint arXiv:1606.05250\\n(2016). 29, 31\\n[363] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\\nK. Toutanova, Boolq: Exploring the surprising di fficulty of natural\\nyes/no questions, arXiv preprint arXiv:1905.10044 (2019). 29, 31\\n[364] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\\nable questions for squad, arXiv preprint arXiv:1806.03822 (2018). 29,\\n31\\n[365] D. Dua, Y . Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\\nA reading comprehension benchmark requiring discrete reasoning over\\nparagraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\\n[366] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\\ntailment challenge, in: Machine learning challenges workshop, Springer,\\n2005, pp. 177–190. 29, 31\\n[367] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y . Bisk, Webqa: Mul-\\ntihop and multimodal qa, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31\\n[368] Y . Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\\nevaluation on chinese machine reading comprehension, arXiv preprint\\narXiv:1709.08299 (2017). 29\\n[369] Y . Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\\nA span-extraction dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:1810.07366 (2018). 29, 31\\n[370] Y . Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\\nA sentence cloze dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:2004.03116 (2020). 29\\n[371] Y . Li, T. Liu, D. Li, Q. Li, J. Shi, Y . Wang, Character-based bilstm-crf\\nincorporating pos and dictionaries for chinese opinion target extraction,\\nin: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29\\n[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\\ning beyond the surface: A challenge set for reading comprehension\\nover multiple sentences, in: Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, V olume 1 (Long Papers), 2018,\\npp. 252–262. 29\\n[373] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\\nberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\\ntions: a benchmark for question answering research, Transactions of the\\nAssociation for Computational Linguistics 7 (2019) 453–466. 29\\n[374] C. C. Shao, T. Liu, Y . Lai, Y . Tseng, S. Tsai, Drcd: A chinese ma-\\nchine reading comprehension dataset, arXiv preprint arXiv:1806.00920\\n(2018). 29\\n[375] W. He, K. Liu, J. Liu, Y . Lyu, S. Zhao, X. Xiao, Y . Liu, Y . Wang, H. Wu,\\nQ. She, et al., Dureader: a chinese machine reading comprehension\\ndataset from real-world applications, arXiv preprint arXiv:1711.05073\\n(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y . Hong, H. Wu, H. Wang, Dureaderrobust: A\\nchinese dataset towards evaluating the robustness of machine reading\\ncomprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\\n[377] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\\nquestions, arXiv preprint arXiv:1707.06209 (2017). 29\\n[378] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\\nranking with kernel pooling, in: Proceedings of the 40th International\\nACM SIGIR conference on research and development in information\\nretrieval, 2017, pp. 55–64. 29\\n[379] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcli ffe, R. Morante,\\nQa4mre 2011-2013: Overview of question answering for machine read-\\ning evaluation, in: Information Access Evaluation. Multilinguality, Mul-\\ntimodality, and Visualization: 4th International Conference of the CLEF\\nInitiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\\nceedings 4, Springer, 2013, pp. 303–320. 29\\n[380] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\\nreading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\\n[381] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y . Feng, X. Han,\\nZ. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\\nment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\\n[382] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\\ncompetence with apps, arXiv preprint arXiv:2105.09938 (2021). 29, 31\\n[383] Y . Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\\nin: Proceedings of the 2017 conference on empirical methods in natural\\nlanguage processing, 2017, pp. 845–854. 29, 31\\n[384] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\\nto solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\\n29, 31\\n[385] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V . Le, C. Sutton, Program synthesis with\\nlarge language models, CoRR abs/2108.07732 (2021). 29\\n[386] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. V osoughi, H. W.\\nChung, Y . Tay, S. Ruder, D. Zhou, et al., Language models are mul-\\ntilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\\n(2022). 29\\n[387] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\\npreprint arXiv:1608.01413 (2016). 29\\n[388] S.-Y . Miao, C.-C. Liang, K.-Y . Su, A diverse corpus for evaluating\\nand developing english math word problem solvers, arXiv preprint\\narXiv:2106.15772 (2021). 29\\n[389] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\\nMawps: A math word problem repository, in: Proceedings of the 2016\\nconference of the north american chapter of the association for computa-\\ntional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29\\n[390] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\\nsimple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\\n29\\n[391] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\\nD. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\\ndata science code generation, in: International Conference on Machine\\nLearning, PMLR, 2023, pp. 18319–18345. 29\\n[392] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\\nlanguage models, arXiv preprint arXiv:2108.07732 (2021). 29\\n[393] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\\nsarial nli: A new benchmark for natural language understanding, arXiv\\npreprint arXiv:1910.14599 (2019). 29, 31\\n[394] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\\ncorpus for sentence understanding through inference, arXiv preprint\\narXiv:1704.05426 (2017). 29\\n[395] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 44, 'page_label': '45'}, page_content='nosing syntactic heuristics in natural language inference, arXiv preprint\\narXiv:1902.01007 (2019). 29\\n[396] J. Liu, L. Cui, H. Liu, D. Huang, Y . Wang, Y . Zhang, Logiqa: A chal-\\nlenge dataset for machine reading comprehension with logical reason-\\ning, arXiv preprint arXiv:2007.08124 (2020). 29\\n[397] P. Lewis, B. O ˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\\nuating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29\\n[398] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, V . Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\\nresentations, arXiv preprint arXiv:1809.05053 (2018). 29, 31\\n[399] Y . Yang, Y . Zhang, C. Tar, J. Baldridge, Paws-x: A cross-\\nlingual adversarial dataset for paraphrase identification, arXiv preprint\\narXiv:1908.11828 (2019). 29, 31\\n[400] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\\nsummary!, Topic-Aware Convolutional Neural Networks for Extreme\\nSummarization. ArXiv, abs (1808). 29\\n[401] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli ´c, A. Korhonen,\\nXcopa: A multilingual dataset for causal commonsense reasoning, arXiv\\npreprint arXiv:2005.00333 (2020). 29\\n[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\\nas a baseline for cross-lingual transfer in commonsense reasoning, arXiv\\npreprint arXiv:2106.12066 (2021). 29\\n[403] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V . Niko-\\nlaev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\\ntion answering in typologically diverse languages, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 454–470. 29\\n[404] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\\nMlsum: The multilingual summarization corpus, arXiv preprint\\narXiv:2004.14900 (2020). 29\\n[405] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\\nhuman falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29, 32\\n[406] I. Augenstein, C. Lioma, D. Wang, L. C. Lima, C. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc: A real-world multi-domain\\ndataset for evidence-based fact checking of claims, arXiv preprint\\narXiv:1909.03242 (2019). 29\\n[407] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\\nlarge-scale dataset for fact extraction and verification, arXiv preprint\\narXiv:1803.05355 (2018). 29\\n[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\\nhate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\\n29, 32\\n[409] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\\nbias in pretrained language models, arXiv preprint arXiv:2004.09456\\n(2020). 29, 32\\n[410] A. Parrish, A. Chen, N. Nangia, V . Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\\nquestion answering, arXiv preprint arXiv:2110.08193 (2021). 29\\n[411] J. Zhao, T. Wang, M. Yatskar, V . Ordonez, K.-W. Chang, Gender bias\\nin coreference resolution: Evaluation and debiasing methods, arXiv\\npreprint arXiv:1804.06876 (2018). 29\\n[412] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\\nlenge dataset for measuring social biases in masked language models,\\narXiv preprint arXiv:2010.00133 (2020). 29\\n[413] S. Gehman, S. Gururangan, M. Sap, Y . Choi, N. A. Smith, Realtoxic-\\nityprompts: Evaluating neural toxic degeneration in language models,\\narXiv preprint arXiv:2009.11462 (2020). 29\\n[414] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\\nmetrics for measuring unintended bias with real data for text classifica-\\ntion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29\\n[415] O. Bojar, R. Chatterjee, C. Federmann, Y . Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V . Logacheva, C. Monz, et al., Find-\\nings of the 2016 conference on machine translation, in: Proceedings of\\nthe First Conference on Machine Translation: V olume 2, Shared Task\\nPapers, 2016, pp. 131–198. 29\\n[416] B. Loïc, B. Magdalena, B. Ond ˇrej, F. Christian, G. Yvette, G. Ro-\\nman, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\\n2020 conference on machine translation (wmt20), in: Proceedings of\\nthe Fifth Conference on Machine Translation, Association for Compu-\\ntational Linguistics„ 2020, pp. 1–55. 29\\n[417] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\\nmatching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\\n[418] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\\nwikipedia: Knowledge-powered conversational agents, arXiv preprint\\narXiv:1811.01241 (2018). 29\\n[419] H. Rashkin, E. M. Smith, M. Li, Y .-L. Boureau, Towards empathetic\\nopen-domain conversation models: A new benchmark and dataset, arXiv\\npreprint arXiv:1811.00207 (2018). 29\\n[420] E. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shuster, J. Urbanek,\\nD. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\\ntional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\\ntion: From Machine Learning to Intelligent Conversations, Springer,\\n2020, pp. 187–208. 29\\n[421] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\\nmulti-domain dialogue dataset towards multi-turn knowledge-driven\\nconversation, arXiv preprint arXiv:2004.04100 (2020). 29\\n[422] L. CO, Iflytek: a multiple categories chinese text classifier. competition\\nofficial website (2019). 29\\n[423] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\\npushshift reddit dataset, in: Proceedings of the international AAAI con-\\nference on web and social media, V ol. 14, 2020, pp. 830–839. 30\\n[424] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\\nform question answering, arXiv preprint arXiv:1907.09190 (2019). 31\\n[425] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\\nBenchmarking generalization via in-context instructions on 1,600+ lan-\\nguage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\\n[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\\nM. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\\ntasking structured knowledge grounding with text-to-text language mod-\\nels, arXiv preprint arXiv:2201.05966 (2022). 31\\n[427] Q. Ye, B. Y . Lin, X. Ren, Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\\n(2021). 31\\n[428] V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta,\\nH. Zhuang, V . Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\\nmulti-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\\n(2021). 31\\n[429] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\\npus for sentence understanding through inference, in: Proceedings of\\nthe 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\nV olume 1 (Long Papers), Association for Computational Linguistics,\\nNew Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\\nN18-1101.\\nURL https://aclanthology.org/N18-1101 31\\n[430] Y . Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\\nscrambling, in: Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, V olume 1 (Long and Short Papers), Associa-\\ntion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\\n1298–1308. doi:10.18653/v1/N19-1131.\\nURL https://aclanthology.org/N19-1131 32\\n[431] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\\nGPT a general-purpose natural language processing task solver?, in: The\\n2023 Conference on Empirical Methods in Natural Language Process-\\ning, 2023.\\nURL https://openreview.net/forum?id=u03xn1COsO 32\\n[432] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\\nprehensive survey of its applications, challenges, limitations, and future\\nprospects, TechRxiv (2023). 32\\n[433] X. L. Dong, S. Moon, Y . E. Xu, K. Malik, Z. Yu, Towards next-\\ngeneration intelligent assistants leveraging llm techniques, in: Proceed-\\nings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, 2023, pp. 5792–5793. 32\\n[434] K. Pandya, M. Holia, Automating customer service using langchain:\\nBuilding custom open-source gpt chatbot for organizations, arXiv\\npreprint arXiv:2310.05421 (2023). 32\\n[435] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\\nR. Geng, et al., Can llm already serve as a database interface? a\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 45, 'page_label': '46'}, page_content='big bench for large-scale database grounded text-to-sqls, arXiv preprint\\narXiv:2305.03111 (2023). 32\\n[436] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\\nchatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\\n2023–02. 32\\n[437] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\\nsir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\\nlanguage models for decision support in personalized oncology, JAMA\\nNetwork Open 6 (11) (2023) e2343689–e2343689. 32\\n[438] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\\nmaroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\\nploring the potential of chat-gpt as a supportive tool for sialendoscopy\\nclinical decision making and patient information support, European\\nArchives of Oto-Rhino-Laryngology (2023) 1–6. 32\\n[439] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\\nData decentralisation of llm-based chatbot systems in chronic disease\\nself-management, in: Proceedings of the 2023 ACM Conference on In-\\nformation Technology for Social Good, 2023, pp. 205–212. 32\\n[440] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\\nhuman feedback for a therapy chatbot application (2023). 32\\n[441] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\\nagents: A personalized llm-powered agent framework, arXiv preprint\\narXiv:2310.02374 (2023). 32\\n[442] K. V . Lemley, Does chatgpt help us understand the medical literature?,\\nJournal of the American Society of Nephrology (2023) 10–1681. 32\\n[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\\nnext-generation large language model (llm) or chatgpt is required for\\nbiomedical engineering and research, Annals of Biomedical Engineering\\n(2023) 1–4. 32\\n[444] Y . Du, S. Zhao, Y . Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\\ncalla dataset: Probing llms’ interactive knowledge acquisition from chi-\\nnese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\\n[445] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\\nS. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\\nlanguage models in medical education: Opportunities, challenges, and\\nfuture directions, JMIR Medical Education 9 (1) (2023) e48291. 32\\n[446] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\\nChatgpt passing usmle shines a spotlight on the flaws of medical educa-\\ntion (2023). 32\\n[447] S. Ahn, The impending impacts of large language models on medical\\neducation, Korean Journal of Medical Education 35 (1) (2023) 103. 32\\n[448] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\\n(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\\n(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\\nArtificial intelligence and public health: Evaluating chatgpt responses to\\nvaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 32\\n[450] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\\nTozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\\nai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32\\n[451] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\\nformance of chatgpt and other large language models (llm) for scientific\\nand research advancements: a double-edged sword, International Re-\\nsearch Journal of Modernization in Engineering Technology and Science\\n5 (10) (2023) 875–899. 32\\n[452] W. Dai, J. Lin, H. Jin, T. Li, Y .-S. Tsai, D. Gaševi´c, G. Chen, Can large\\nlanguage models provide feedback to students? a case study on chatgpt,\\nin: 2023 IEEE International Conference on Advanced Learning Tech-\\nnologies (ICALT), IEEE, 2023, pp. 323–325. 32\\n[453] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\\nChatgpt for good? on opportunities and challenges of large language\\nmodels for education, Learning and individual di fferences 103 (2023)\\n102274. 32\\n[454] N. Rane, Enhancing the quality of teaching and learning through chat-\\ngpt and similar large language models: Challenges, future prospects,\\nand ethical considerations in education, Future Prospects, and Ethical\\nConsiderations in Education (September 15, 2023) (2023). 32\\n[455] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\\ngenerating chatbot’s dialogue for english as a foreign language learning,\\nInternational Journal of Advanced Computer Science and Applications\\n14 (6) (2023). 32\\n[456] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\\nthe impacts of chatgpt on future scientific work, SocArXiv (2023). 32\\n[457] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\\nscholarly writing: Is the integrity of the scientific discourse in jeopardy?,\\narXiv preprint arXiv:2311.06981 (2023). 32\\n[458] Y . Zheng, H. Y . Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\\nLarge language models for scientific synthesis, inference and explana-\\ntion, arXiv preprint arXiv:2310.07984 (2023). 33\\n[459] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\\nin scientific writing, PsyArXiv (2023). 33\\n[460] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\\nentific writing: a friend or a foe?, Reproductive BioMedicine Online\\n(2023). 33\\n[461] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\\nusing large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33\\n[462] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\\non learning mathematical reasoning with large language models, arXiv\\npreprint arXiv:2308.01825 (2023). 33\\n[463] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\\nR. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\\naugmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33\\n[464] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\\nT. Lukasiewicz, Y . Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\\nlanguage models for mathematics through interactions, arXiv preprint\\narXiv:2306.01694 (2023). 33\\n[465] Y . Liu, T. Han, S. Ma, J. Zhang, Y . Yang, J. Tian, H. He, A. Li, M. He,\\nZ. Liu, et al., Summary of chatgpt-related research and perspective\\ntowards the future of large language models, Meta-Radiology (2023)\\n100017. 33\\n[466] J. Drápal, H. Westermann, J. Savelka, Using large language models\\nto support thematic analysis in empirical legal studies, arXiv preprint\\narXiv:2310.18729 (2023). 33\\n[467] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\\ning legal concepts with augmented large language models (gpt-4), arXiv\\npreprint arXiv:2306.09525 (2023). 33\\n[468] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\\nA. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\\nbench: A collaboratively built benchmark for measuring legal reasoning\\nin large language models, arXiv preprint arXiv:2308.11462 (2023). 33\\n[469] J. Cui, Z. Li, Y . Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases, arXiv\\npreprint arXiv:2306.16092 (2023). 33\\n[470] H. Yang, X.-Y . Liu, C. D. Wang, Fingpt: Open-source financial large\\nlanguage models, arXiv preprint arXiv:2306.06031 (2023). 33\\n[471] Y . Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\\nsurvey, in: Proceedings of the Fourth ACM International Conference on\\nAI in Finance, 2023, pp. 374–382. 33\\n[472] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model, arXiv preprint\\narXiv:2305.19352 (2023). 33\\n[473] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\\naction, in: ACM/IEEE International Conference on Human-Robot Inter-\\naction, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33\\n[474] Y . Ye, H. You, J. Du, Improved trust in human-robot collaboration with\\nchatgpt, IEEE Access (2023). 33\\n[475] Y . Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\\nknowledge from large language models for task and motion planning,\\nin: RSS 2023 Workshop on Learning for Task and Motion Planning,\\n2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\\nwith large language models, arXiv preprint arXiv:2305.05658 (2023).\\n33\\n[477] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\\nfor deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 34\\n[478] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'A Comprehensive Overview of Large Language Models', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 47, 'page': 46, 'page_label': '47'}, page_content='gers of stochastic parrots: Can language models be too big?, in: Pro-\\nceedings of the 2021 ACM conference on fairness, accountability, and\\ntransparency, 2021, pp. 610–623. 34\\n[479] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\\ndeep learning (still) requires rethinking generalization, Communications\\nof the ACM 64 (3) (2021) 107–115. 34\\n[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\\ntrained language models, arXiv preprint arXiv:2105.00828 (2021). 34\\n[481] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\\nNow (2019) 1–33. 34\\n[482] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\\nguage models still can’t plan (a benchmark for llms on planning and\\nreasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\\n[483] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chen, et al., Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models, arXiv preprint arXiv:2309.01219\\n(2023). 34\\n[484] A. Webson, E. Pavlick, Do prompt-based models really understand the\\nmeaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\\n[485] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\\nthought, let’s not think step by step! bias and toxicity in zero-shot rea-\\nsoning, arXiv preprint arXiv:2212.08061 (2022). 34\\n[486] B. C. Das, M. H. Amini, Y . Wu, Security and privacy challenges of large\\nlanguage models: A survey, arXiv preprint arXiv:2402.00888 (2024). 34\\n[487] X. Liu, H. Cheng, P. He, W. Chen, Y . Wang, H. Poon, J. Gao, Adversar-\\nial training for large neural language models, ArXiv (April 2020).\\nURL https://www.microsoft.com/en-us/research/\\npublication/adversarial-training-for-large-neural-language-models/\\n34\\n[488] E. Shayegani, M. A. A. Mamun, Y . Fu, P. Zaree, Y . Dong, N. Abu-\\nGhazaleh, Survey of vulnerabilities in large language models revealed\\nby adversarial attacks (2023). arXiv:2310.10844. 34\\n[489] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\\nllm can fool itself: A prompt-based adversarial attack (2023). arXiv:\\n2310.13345. 34\\n[490] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nM. Du, Explainability for large language models: A survey (2023).\\narXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y . Zhou, L. H. Gilpin, Can large\\nlanguage models explain themselves? a study of llm-generated self-\\nexplanations (2023). arXiv:2310.11207. 35\\n[492] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\\nmean for a language model to preserve privacy?, in: Proceedings of the\\n2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35\\n[493] R. Plant, V . Giu ffrida, D. Gkatzia, You are what you write: Pre-\\nserving privacy in the era of large language models, arXiv preprint\\narXiv:2204.09391 (2022). 35\\n[494] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\\nB. Ren, Y . Wang, Real-time execution of large-scale language models\\non mobile (2020). arXiv:2009.06823. 35\\n[495] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y . Liu, M. Guo,\\nY . Zhu, Olive: Accelerating large language models via hardware-\\nfriendly outlier-victim pair quantization, in: Proceedings of the 50th\\nAnnual International Symposium on Computer Architecture, 2023, pp.\\n1–15. 35\\n[496] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\\nlanguage models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35\\n[497] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\\nand policy implications for large language models: Guiding responsible\\ndevelopment and deployment, arXiv preprint arXiv:2308.02678 (2023).\\n35\\n[498] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\\nmodels: a three-layered approach, AI and Ethics (2023) 1–31. 35\\n47')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a PDF File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader('attention.pdf')\n",
    "pdf_document = pdf_loader.load()\n",
    "pdf_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d16ae7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7ba9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_document[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1418e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.wexdisoftwareservices.com/'}, page_content='\\n\\n    We use cookies to improve your experience on our website.\\n    By continuing, you agree to our Cookie Policy.\\n  \\n\\nAccept all cookies\\nCustomize cookies\\n\\n')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "web_loader = WebBaseLoader(web_path=(\"https://www.wexdisoftwareservices.com/\", ), \n",
    "                           bs_kwargs=dict(parse_only = bs4.SoupStrainer(\n",
    "                               class_=(\"cookie-banner\", \"container-fluid\")\n",
    "                               \n",
    "                           ))\n",
    "                           )\n",
    "web_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21cf75d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv Loader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs = 2).load()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06aa7154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e177eadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "wiki_docs = WikipediaLoader(query=\"MS Dhoni\", load_max_docs=2).load()\n",
    "len(wiki_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7723404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ([məˈɦeːnd̪ɾə ˈsɪŋɡʱ ˈd̪ʱoːniː] ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010 and 2016, and he was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011. In June 2025, he was inducted into ICC Cricket Hall of Fame.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='Mahendra Singh Dhoni ([məˈɦeːnd̪ɾə ˈsɪŋɡʱ ˈd̪ʱoːniː] ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010 and 2016, and he was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India\\'s highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011. In June 2025, he was inducted into ICC Cricket Hall of Fame.\\n\\n\\n== Early life ==\\nDhoni was born on 7 July 1981 in Ranchi, Bihar (now in Jharkhand) in a Hindu Rajput family to Pan Singh and Devaki Devi. His parents hailed from Lwali village in Uttar Pradesh (now Uttarakhand) and he was the youngest of three children. His family spells the surname as \"Dhauni\". The spelling \"Dhoni\" emerged due to a spelling mistake in his school certificates and, despite repeated attempts by his family, has never been rectified.\\nDhoni did his schooling from DAV Jawahar Vidya Mandir, where he started playing football as a goalkeeper, but later moved to play cricket on the suggestion of his coach Keshav Banerjee. From 2001 to 2003, Dhoni worked as a Travelling Ticket Examiner (TTE) at Kharagpur under South Eastern Railway zone of Indian Railways.\\n\\n\\n== Youth career ==\\nHe played as a wicket-keeper for Commando cricket club from 1995 to 1998 and Central Coal Fields Limited (CCL) team in 1998. At CCL, he batted higher up the order and helped the team qualify to the higher division. Based on his performance at club cricket, he was picked for the 1997/98 season of Vinoo Mankad Trophy under-16 championship. In the 1998–99, Dhoni played for Bihar U-19 team in the Cooch Behar Trophy and scored 176 runs in 5 matches. In the 1999–2000 Cooch Behar Trophy, the Bihar U-19 cricket team made it to the finals, where Dhoni made 84 in a losing cause. Dhoni\\'s contribution in the tournament included 488 runs in nine matches with five fifties, 17 catches and seven stumpings. Dhoni made it to the East Zone U-19 squad for the C. K. Nayudu Trophy in the 1999–2000 season and scored only 97 runs in four matches, as East Zone lost all the matches and finished last in the tournament.\\nDhoni made his Ranji Trophy debut for Bihar against Assam in t'),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\\n== Plot ==\\nIn the pre-credits sequence there is a scene of the 2011 Cricket World Cup Final. MS Dhoni, India's captain, walks out to bat after Virat Kohli's wicket.\\nThe film begins in Ranchi, 7 July 1981. At the hospital maternity unit, Paan Singh Dhoni is confused whether he has got a girl or boy. He later names his baby boy Mahendra 'Mahi' Singh Dhoni. Paan Singh is a pump operator who waters the practice ground. Fourteen years later, Mahi is spotted by a cricket coach while goalkeeping in a football game. He invites him to try out for the school cricket team as a wicketkeeper and selects him after being impressed. Mahi improves his batting and becomes a regular member of the team.\\nThree years later, a grown up Mahi helps win an inter-school cricket match. After achieving much fame, Mahi is selected for the Ranji Trophy but his draft notice is held up due to which he is late in reaching Kolkata despite his friends' help. But Mahi does not give up and, to please his father, he joins the Kharagpur Station as a ticket collector. Years later, Mahi's sister Jayanti is married to his friend Gautam Gupta.\\nAfter some time, Mahi is depressed with his job. With the insistence of his manager, Mahi decides to play cricket alongside his work, and after his day-shifts he goes to practice cricket. He participates in different tournaments and as a result he gets selected for the Railways. After a good performance, he tries-out for the India national under-19 cricket team selections. Bihar loses to Punjab where Yuvraj Singh scores 301 and Mahi does not succeed though he is selected for the Duleep Trophy.\\nMahi leaves his job and admits to his father that cricket is his only ambition and he wants to become a professional cricketer. He works hard and is selected in the national team and makes his debut. He meets and befriends Priyanka Jha, an office consultant, and scores a century after meeting her. She buys a watch for him as a Valentine's Day gift but dies in a truck accident on her way. Mahi again goes into depression and has bad form in the 2007 Cricket World Cup. As captain of the national side, he wins the T-20 World Cup, and leads India to the number one ranking in Test matches.\\nIn 2010, Mahi arrives at a hotel. Sakshi Singh Rawat, a hotel intern (catering management student) fails to recognize him and later apologizes to him. They soon start dating and Mahi eventually proposes marriage to her after she mentions buying him a Valentine's Day's gift which he refuses. They marry and Mahi begins training for the 2011 World Cup. He eventually d\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
